
================== checkpoint0119.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0119.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:17:33  class_error: 57.78  loss: 14.2332 (14.2332)  loss_ce: 10.7619 (10.7619)  loss_bbox: 2.1255 (2.1255)  loss_giou: 1.2547 (1.2547)  loss_conf: 0.0912 (0.0912)  loss_ce_unscaled: 2.1524 (2.1524)  class_error_unscaled: 57.7778 (57.7778)  loss_bbox_unscaled: 0.4251 (0.4251)  loss_giou_unscaled: 0.6274 (0.6274)  loss_conf_unscaled: 0.0091 (0.0091)  time: 4.9467  data: 2.1738  max mem: 16066
Test:  [ 10/213]  eta: 0:03:36  class_error: 52.13  loss: 14.2332 (14.8931)  loss_ce: 10.8088 (11.3017)  loss_bbox: 2.2558 (2.2241)  loss_giou: 1.2639 (1.2789)  loss_conf: 0.0841 (0.0884)  loss_ce_unscaled: 2.1618 (2.2603)  class_error_unscaled: 63.7500 (61.8009)  loss_bbox_unscaled: 0.4512 (0.4448)  loss_giou_unscaled: 0.6320 (0.6395)  loss_conf_unscaled: 0.0084 (0.0088)  time: 1.0684  data: 0.3498  max mem: 16066
Test:  [ 20/213]  eta: 0:02:59  class_error: 74.23  loss: 14.9528 (14.8559)  loss_ce: 11.4415 (11.3569)  loss_bbox: 2.0899 (2.1681)  loss_giou: 1.2415 (1.2400)  loss_conf: 0.0798 (0.0908)  loss_ce_unscaled: 2.2883 (2.2714)  class_error_unscaled: 64.0449 (62.7550)  loss_bbox_unscaled: 0.4180 (0.4336)  loss_giou_unscaled: 0.6207 (0.6200)  loss_conf_unscaled: 0.0080 (0.0091)  time: 0.7297  data: 0.1905  max mem: 16066
Test:  [ 30/213]  eta: 0:02:38  class_error: 60.23  loss: 14.9940 (14.7497)  loss_ce: 11.5578 (11.3096)  loss_bbox: 2.0782 (2.1373)  loss_giou: 1.2004 (1.2178)  loss_conf: 0.0726 (0.0851)  loss_ce_unscaled: 2.3116 (2.2619)  class_error_unscaled: 64.6465 (62.8407)  loss_bbox_unscaled: 0.4156 (0.4275)  loss_giou_unscaled: 0.6002 (0.6089)  loss_conf_unscaled: 0.0073 (0.0085)  time: 0.7574  data: 0.1954  max mem: 16066
Test:  [ 40/213]  eta: 0:02:23  class_error: 59.57  loss: 14.4060 (14.6441)  loss_ce: 10.9093 (11.2228)  loss_bbox: 2.0782 (2.1278)  loss_giou: 1.1908 (1.2108)  loss_conf: 0.0715 (0.0827)  loss_ce_unscaled: 2.1819 (2.2446)  class_error_unscaled: 64.0449 (63.0987)  loss_bbox_unscaled: 0.4156 (0.4256)  loss_giou_unscaled: 0.5954 (0.6054)  loss_conf_unscaled: 0.0072 (0.0083)  time: 0.7206  data: 0.1655  max mem: 16066
Test:  [ 50/213]  eta: 0:02:12  class_error: 57.45  loss: 14.1156 (14.6214)  loss_ce: 10.7884 (11.2068)  loss_bbox: 2.0688 (2.1207)  loss_giou: 1.1932 (1.2108)  loss_conf: 0.0799 (0.0831)  loss_ce_unscaled: 2.1577 (2.2414)  class_error_unscaled: 62.0690 (62.5671)  loss_bbox_unscaled: 0.4138 (0.4241)  loss_giou_unscaled: 0.5966 (0.6054)  loss_conf_unscaled: 0.0080 (0.0083)  time: 0.7210  data: 0.1569  max mem: 16066
Test:  [ 60/213]  eta: 0:02:01  class_error: 59.14  loss: 14.0052 (14.5559)  loss_ce: 10.8478 (11.1600)  loss_bbox: 2.0009 (2.1034)  loss_giou: 1.2184 (1.2097)  loss_conf: 0.0801 (0.0829)  loss_ce_unscaled: 2.1696 (2.2320)  class_error_unscaled: 58.3333 (61.9659)  loss_bbox_unscaled: 0.4002 (0.4207)  loss_giou_unscaled: 0.6092 (0.6048)  loss_conf_unscaled: 0.0080 (0.0083)  time: 0.7300  data: 0.1582  max mem: 16066
Test:  [ 70/213]  eta: 0:01:53  class_error: 58.43  loss: 13.8453 (14.5851)  loss_ce: 10.8478 (11.1926)  loss_bbox: 2.0009 (2.1016)  loss_giou: 1.2184 (1.2072)  loss_conf: 0.0839 (0.0837)  loss_ce_unscaled: 2.1696 (2.2385)  class_error_unscaled: 59.1398 (61.8475)  loss_bbox_unscaled: 0.4002 (0.4203)  loss_giou_unscaled: 0.6092 (0.6036)  loss_conf_unscaled: 0.0084 (0.0084)  time: 0.7442  data: 0.1615  max mem: 16066
Test:  [ 80/213]  eta: 0:01:44  class_error: 51.06  loss: 14.3848 (14.6054)  loss_ce: 11.0056 (11.2151)  loss_bbox: 2.0300 (2.0973)  loss_giou: 1.2092 (1.2086)  loss_conf: 0.0873 (0.0845)  loss_ce_unscaled: 2.2011 (2.2430)  class_error_unscaled: 60.8696 (62.0214)  loss_bbox_unscaled: 0.4060 (0.4195)  loss_giou_unscaled: 0.6046 (0.6043)  loss_conf_unscaled: 0.0087 (0.0084)  time: 0.7524  data: 0.1680  max mem: 16066
Test:  [ 90/213]  eta: 0:01:36  class_error: 54.44  loss: 14.8483 (14.6605)  loss_ce: 11.1151 (11.2469)  loss_bbox: 2.1504 (2.1144)  loss_giou: 1.2554 (1.2149)  loss_conf: 0.0740 (0.0844)  loss_ce_unscaled: 2.2230 (2.2494)  class_error_unscaled: 61.2245 (62.1365)  loss_bbox_unscaled: 0.4301 (0.4229)  loss_giou_unscaled: 0.6277 (0.6074)  loss_conf_unscaled: 0.0074 (0.0084)  time: 0.7483  data: 0.1629  max mem: 16066
Test:  [100/213]  eta: 0:01:27  class_error: 62.37  loss: 14.9596 (14.7209)  loss_ce: 11.6827 (11.2932)  loss_bbox: 2.2645 (2.1242)  loss_giou: 1.2554 (1.2185)  loss_conf: 0.0740 (0.0850)  loss_ce_unscaled: 2.3365 (2.2586)  class_error_unscaled: 62.3656 (62.2542)  loss_bbox_unscaled: 0.4529 (0.4248)  loss_giou_unscaled: 0.6277 (0.6092)  loss_conf_unscaled: 0.0074 (0.0085)  time: 0.7245  data: 0.1590  max mem: 16066
Test:  [110/213]  eta: 0:01:19  class_error: 64.13  loss: 14.4666 (14.6773)  loss_ce: 10.6371 (11.2409)  loss_bbox: 2.2037 (2.1303)  loss_giou: 1.2500 (1.2226)  loss_conf: 0.0716 (0.0836)  loss_ce_unscaled: 2.1274 (2.2482)  class_error_unscaled: 64.1304 (62.1727)  loss_bbox_unscaled: 0.4407 (0.4261)  loss_giou_unscaled: 0.6250 (0.6113)  loss_conf_unscaled: 0.0072 (0.0084)  time: 0.7400  data: 0.1584  max mem: 16066
Test:  [120/213]  eta: 0:01:12  class_error: 67.44  loss: 14.6704 (14.7178)  loss_ce: 11.1303 (11.2707)  loss_bbox: 2.1417 (2.1371)  loss_giou: 1.2742 (1.2276)  loss_conf: 0.0653 (0.0824)  loss_ce_unscaled: 2.2261 (2.2541)  class_error_unscaled: 63.6364 (62.3088)  loss_bbox_unscaled: 0.4283 (0.4274)  loss_giou_unscaled: 0.6371 (0.6138)  loss_conf_unscaled: 0.0065 (0.0082)  time: 0.7849  data: 0.1545  max mem: 16066
Test:  [130/213]  eta: 0:01:03  class_error: 62.37  loss: 15.0368 (14.7391)  loss_ce: 11.4084 (11.2804)  loss_bbox: 2.2382 (2.1446)  loss_giou: 1.2870 (1.2328)  loss_conf: 0.0653 (0.0814)  loss_ce_unscaled: 2.2817 (2.2561)  class_error_unscaled: 63.6364 (62.6351)  loss_bbox_unscaled: 0.4476 (0.4289)  loss_giou_unscaled: 0.6435 (0.6164)  loss_conf_unscaled: 0.0065 (0.0081)  time: 0.7520  data: 0.1717  max mem: 16066
Test:  [140/213]  eta: 0:00:56  class_error: 63.92  loss: 14.4103 (14.7237)  loss_ce: 11.0332 (11.2719)  loss_bbox: 2.1419 (2.1404)  loss_giou: 1.2119 (1.2297)  loss_conf: 0.0761 (0.0817)  loss_ce_unscaled: 2.2066 (2.2544)  class_error_unscaled: 62.9213 (62.2948)  loss_bbox_unscaled: 0.4284 (0.4281)  loss_giou_unscaled: 0.6059 (0.6149)  loss_conf_unscaled: 0.0076 (0.0082)  time: 0.7621  data: 0.1728  max mem: 16066
Test:  [150/213]  eta: 0:00:48  class_error: 65.56  loss: 14.2046 (14.7023)  loss_ce: 10.5343 (11.2456)  loss_bbox: 2.1385 (2.1424)  loss_giou: 1.2119 (1.2318)  loss_conf: 0.0846 (0.0824)  loss_ce_unscaled: 2.1069 (2.2491)  class_error_unscaled: 60.2041 (62.1729)  loss_bbox_unscaled: 0.4277 (0.4285)  loss_giou_unscaled: 0.6059 (0.6159)  loss_conf_unscaled: 0.0085 (0.0082)  time: 0.7349  data: 0.1544  max mem: 16066
Test:  [160/213]  eta: 0:00:40  class_error: 54.35  loss: 14.6935 (14.7070)  loss_ce: 11.0750 (11.2537)  loss_bbox: 2.1658 (2.1388)  loss_giou: 1.2375 (1.2317)  loss_conf: 0.0846 (0.0827)  loss_ce_unscaled: 2.2150 (2.2507)  class_error_unscaled: 60.8247 (62.0627)  loss_bbox_unscaled: 0.4332 (0.4278)  loss_giou_unscaled: 0.6187 (0.6158)  loss_conf_unscaled: 0.0085 (0.0083)  time: 0.7453  data: 0.1522  max mem: 16066
Test:  [170/213]  eta: 0:00:32  class_error: 54.02  loss: 14.7287 (14.6734)  loss_ce: 11.0750 (11.2280)  loss_bbox: 2.1207 (2.1339)  loss_giou: 1.2003 (1.2288)  loss_conf: 0.0816 (0.0826)  loss_ce_unscaled: 2.2150 (2.2456)  class_error_unscaled: 60.8247 (61.8792)  loss_bbox_unscaled: 0.4241 (0.4268)  loss_giou_unscaled: 0.6002 (0.6144)  loss_conf_unscaled: 0.0082 (0.0083)  time: 0.7557  data: 0.1556  max mem: 16066
Test:  [180/213]  eta: 0:00:25  class_error: 60.00  loss: 14.4388 (14.6823)  loss_ce: 11.1233 (11.2377)  loss_bbox: 2.0548 (2.1315)  loss_giou: 1.2148 (1.2301)  loss_conf: 0.0737 (0.0829)  loss_ce_unscaled: 2.2247 (2.2475)  class_error_unscaled: 60.3960 (61.9924)  loss_bbox_unscaled: 0.4110 (0.4263)  loss_giou_unscaled: 0.6074 (0.6150)  loss_conf_unscaled: 0.0074 (0.0083)  time: 0.7725  data: 0.1606  max mem: 16066
Test:  [190/213]  eta: 0:00:17  class_error: 69.57  loss: 14.6197 (14.6890)  loss_ce: 11.4021 (11.2454)  loss_bbox: 2.0345 (2.1306)  loss_giou: 1.2353 (1.2298)  loss_conf: 0.0863 (0.0833)  loss_ce_unscaled: 2.2804 (2.2491)  class_error_unscaled: 64.0000 (62.0885)  loss_bbox_unscaled: 0.4069 (0.4261)  loss_giou_unscaled: 0.6177 (0.6149)  loss_conf_unscaled: 0.0086 (0.0083)  time: 0.7716  data: 0.1655  max mem: 16066
Test:  [200/213]  eta: 0:00:09  class_error: 69.23  loss: 14.4091 (14.6773)  loss_ce: 11.2170 (11.2349)  loss_bbox: 2.0676 (2.1296)  loss_giou: 1.2071 (1.2294)  loss_conf: 0.0832 (0.0835)  loss_ce_unscaled: 2.2434 (2.2470)  class_error_unscaled: 63.7363 (62.1497)  loss_bbox_unscaled: 0.4135 (0.4259)  loss_giou_unscaled: 0.6036 (0.6147)  loss_conf_unscaled: 0.0083 (0.0083)  time: 0.7098  data: 0.1832  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 59.09  loss: 14.5586 (14.7031)  loss_ce: 11.2170 (11.2664)  loss_bbox: 2.0676 (2.1248)  loss_giou: 1.2099 (1.2288)  loss_conf: 0.0775 (0.0831)  loss_ce_unscaled: 2.2434 (2.2533)  class_error_unscaled: 62.6263 (62.1664)  loss_bbox_unscaled: 0.4135 (0.4250)  loss_giou_unscaled: 0.6050 (0.6144)  loss_conf_unscaled: 0.0078 (0.0083)  time: 0.8049  data: 0.1898  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 62.96  loss: 14.5162 (14.7086)  loss_ce: 11.2170 (11.2726)  loss_bbox: 2.0061 (2.1238)  loss_giou: 1.2099 (1.2290)  loss_conf: 0.0775 (0.0831)  loss_ce_unscaled: 2.2434 (2.2545)  class_error_unscaled: 62.9630 (62.2266)  loss_bbox_unscaled: 0.4012 (0.4248)  loss_giou_unscaled: 0.6050 (0.6145)  loss_conf_unscaled: 0.0078 (0.0083)  time: 0.7733  data: 0.1733  max mem: 16066
Test: Total time: 0:02:43 (0.7681 s / it)
Averaged stats: class_error: 62.96  loss: 14.5162 (14.7086)  loss_ce: 11.2170 (11.2726)  loss_bbox: 2.0061 (2.1238)  loss_giou: 1.2099 (1.2290)  loss_conf: 0.0775 (0.0831)  loss_ce_unscaled: 2.2434 (2.2545)  class_error_unscaled: 62.9630 (62.2266)  loss_bbox_unscaled: 0.4012 (0.4248)  loss_giou_unscaled: 0.6050 (0.6145)  loss_conf_unscaled: 0.0078 (0.0083)
zero-shot mAP: 11.79
rare mAP: 16.97
nonrare mAP: 24.42
full mAP: 17.43

================== checkpoint0119.pth 结束：2025-09-11 09:19:41.941342 ==================

================== checkpoint0115.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0115.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:20:51  class_error: 58.89  loss: 13.9779 (13.9779)  loss_ce: 10.5087 (10.5087)  loss_bbox: 2.1223 (2.1223)  loss_giou: 1.2558 (1.2558)  loss_conf: 0.0910 (0.0910)  loss_ce_unscaled: 2.1017 (2.1017)  class_error_unscaled: 58.8889 (58.8889)  loss_bbox_unscaled: 0.4245 (0.4245)  loss_giou_unscaled: 0.6279 (0.6279)  loss_conf_unscaled: 0.0091 (0.0091)  time: 5.8741  data: 2.5281  max mem: 16066
Test:  [ 10/213]  eta: 0:04:41  class_error: 52.13  loss: 13.9779 (14.6747)  loss_ce: 10.6197 (11.0820)  loss_bbox: 2.2576 (2.2248)  loss_giou: 1.2620 (1.2797)  loss_conf: 0.0839 (0.0883)  loss_ce_unscaled: 2.1239 (2.2164)  class_error_unscaled: 63.7500 (61.6065)  loss_bbox_unscaled: 0.4515 (0.4450)  loss_giou_unscaled: 0.6310 (0.6398)  loss_conf_unscaled: 0.0084 (0.0088)  time: 1.3843  data: 0.5662  max mem: 16066
Test:  [ 20/213]  eta: 0:03:50  class_error: 73.20  loss: 14.7672 (14.6327)  loss_ce: 11.2961 (11.1351)  loss_bbox: 2.0888 (2.1654)  loss_giou: 1.2503 (1.2407)  loss_conf: 0.0801 (0.0915)  loss_ce_unscaled: 2.2592 (2.2270)  class_error_unscaled: 64.0449 (62.5432)  loss_bbox_unscaled: 0.4178 (0.4331)  loss_giou_unscaled: 0.6251 (0.6203)  loss_conf_unscaled: 0.0080 (0.0092)  time: 0.9606  data: 0.2999  max mem: 16066
Test:  [ 30/213]  eta: 0:03:17  class_error: 60.23  loss: 14.7073 (14.5217)  loss_ce: 11.2525 (11.0829)  loss_bbox: 2.0753 (2.1359)  loss_giou: 1.1920 (1.2171)  loss_conf: 0.0725 (0.0858)  loss_ce_unscaled: 2.2505 (2.2166)  class_error_unscaled: 64.2857 (62.7326)  loss_bbox_unscaled: 0.4151 (0.4272)  loss_giou_unscaled: 0.5960 (0.6086)  loss_conf_unscaled: 0.0073 (0.0086)  time: 0.9121  data: 0.2311  max mem: 16066
Test:  [ 40/213]  eta: 0:02:56  class_error: 59.57  loss: 14.1934 (14.4106)  loss_ce: 10.7396 (10.9903)  loss_bbox: 2.0753 (2.1268)  loss_giou: 1.1895 (1.2105)  loss_conf: 0.0716 (0.0830)  loss_ce_unscaled: 2.1479 (2.1981)  class_error_unscaled: 64.0449 (62.9891)  loss_bbox_unscaled: 0.4151 (0.4254)  loss_giou_unscaled: 0.5947 (0.6053)  loss_conf_unscaled: 0.0072 (0.0083)  time: 0.8322  data: 0.2183  max mem: 16066
Test:  [ 50/213]  eta: 0:02:43  class_error: 57.45  loss: 13.9267 (14.3895)  loss_ce: 10.4936 (10.9743)  loss_bbox: 2.0719 (2.1212)  loss_giou: 1.1930 (1.2109)  loss_conf: 0.0767 (0.0830)  loss_ce_unscaled: 2.0987 (2.1949)  class_error_unscaled: 61.1765 (62.4176)  loss_bbox_unscaled: 0.4144 (0.4242)  loss_giou_unscaled: 0.5965 (0.6055)  loss_conf_unscaled: 0.0077 (0.0083)  time: 0.8803  data: 0.2022  max mem: 16066
Test:  [ 60/213]  eta: 0:02:29  class_error: 58.06  loss: 13.7143 (14.3261)  loss_ce: 10.5683 (10.9302)  loss_bbox: 2.0004 (2.1032)  loss_giou: 1.2188 (1.2099)  loss_conf: 0.0801 (0.0828)  loss_ce_unscaled: 2.1137 (2.1860)  class_error_unscaled: 57.7778 (61.8404)  loss_bbox_unscaled: 0.4001 (0.4206)  loss_giou_unscaled: 0.6094 (0.6050)  loss_conf_unscaled: 0.0080 (0.0083)  time: 0.8898  data: 0.2381  max mem: 16066
Test:  [ 70/213]  eta: 0:02:18  class_error: 58.43  loss: 13.6668 (14.3499)  loss_ce: 10.6004 (10.9571)  loss_bbox: 2.0004 (2.1018)  loss_giou: 1.2173 (1.2075)  loss_conf: 0.0839 (0.0836)  loss_ce_unscaled: 2.1201 (2.1914)  class_error_unscaled: 58.5366 (61.7395)  loss_bbox_unscaled: 0.4001 (0.4204)  loss_giou_unscaled: 0.6086 (0.6037)  loss_conf_unscaled: 0.0084 (0.0084)  time: 0.8900  data: 0.2666  max mem: 16066
Test:  [ 80/213]  eta: 0:02:08  class_error: 51.06  loss: 14.2138 (14.3754)  loss_ce: 10.8346 (10.9839)  loss_bbox: 2.0308 (2.0981)  loss_giou: 1.2094 (1.2091)  loss_conf: 0.0839 (0.0843)  loss_ce_unscaled: 2.1669 (2.1968)  class_error_unscaled: 60.0000 (61.9266)  loss_bbox_unscaled: 0.4062 (0.4196)  loss_giou_unscaled: 0.6047 (0.6045)  loss_conf_unscaled: 0.0084 (0.0084)  time: 0.9332  data: 0.2667  max mem: 16066
Test:  [ 90/213]  eta: 0:01:57  class_error: 55.56  loss: 14.6506 (14.4274)  loss_ce: 10.9328 (11.0129)  loss_bbox: 2.1519 (2.1150)  loss_giou: 1.2479 (1.2151)  loss_conf: 0.0752 (0.0845)  loss_ce_unscaled: 2.1866 (2.2026)  class_error_unscaled: 61.2245 (62.0653)  loss_bbox_unscaled: 0.4304 (0.4230)  loss_giou_unscaled: 0.6239 (0.6075)  loss_conf_unscaled: 0.0075 (0.0084)  time: 0.9120  data: 0.2492  max mem: 16066
Test:  [100/213]  eta: 0:01:47  class_error: 62.37  loss: 14.7196 (14.4839)  loss_ce: 11.4596 (11.0553)  loss_bbox: 2.2648 (2.1248)  loss_giou: 1.2536 (1.2187)  loss_conf: 0.0793 (0.0851)  loss_ce_unscaled: 2.2919 (2.2111)  class_error_unscaled: 62.3656 (62.1705)  loss_bbox_unscaled: 0.4530 (0.4250)  loss_giou_unscaled: 0.6268 (0.6094)  loss_conf_unscaled: 0.0079 (0.0085)  time: 0.8862  data: 0.2555  max mem: 16066
Test:  [110/213]  eta: 0:01:37  class_error: 64.13  loss: 14.2681 (14.4417)  loss_ce: 10.4379 (11.0069)  loss_bbox: 2.1997 (2.1287)  loss_giou: 1.2510 (1.2222)  loss_conf: 0.0736 (0.0839)  loss_ce_unscaled: 2.0876 (2.2014)  class_error_unscaled: 64.1304 (62.1066)  loss_bbox_unscaled: 0.4399 (0.4257)  loss_giou_unscaled: 0.6255 (0.6111)  loss_conf_unscaled: 0.0074 (0.0084)  time: 0.8878  data: 0.2566  max mem: 16066
Test:  [120/213]  eta: 0:01:27  class_error: 67.44  loss: 14.4938 (14.4827)  loss_ce: 10.8963 (11.0362)  loss_bbox: 2.1375 (2.1364)  loss_giou: 1.2728 (1.2275)  loss_conf: 0.0669 (0.0826)  loss_ce_unscaled: 2.1793 (2.2072)  class_error_unscaled: 64.1304 (62.3017)  loss_bbox_unscaled: 0.4275 (0.4273)  loss_giou_unscaled: 0.6364 (0.6137)  loss_conf_unscaled: 0.0067 (0.0083)  time: 0.8750  data: 0.1996  max mem: 16066
Test:  [130/213]  eta: 0:01:16  class_error: 62.37  loss: 14.8149 (14.5090)  loss_ce: 11.1568 (11.0515)  loss_bbox: 2.2083 (2.1434)  loss_giou: 1.2911 (1.2324)  loss_conf: 0.0646 (0.0817)  loss_ce_unscaled: 2.2314 (2.2103)  class_error_unscaled: 64.7727 (62.6463)  loss_bbox_unscaled: 0.4417 (0.4287)  loss_giou_unscaled: 0.6456 (0.6162)  loss_conf_unscaled: 0.0065 (0.0082)  time: 0.7855  data: 0.1755  max mem: 16066
Test:  [140/213]  eta: 0:01:07  class_error: 63.92  loss: 14.4574 (14.4947)  loss_ce: 10.8525 (11.0436)  loss_bbox: 2.1309 (2.1396)  loss_giou: 1.2117 (1.2296)  loss_conf: 0.0760 (0.0819)  loss_ce_unscaled: 2.1705 (2.2087)  class_error_unscaled: 63.3333 (62.2976)  loss_bbox_unscaled: 0.4262 (0.4279)  loss_giou_unscaled: 0.6059 (0.6148)  loss_conf_unscaled: 0.0076 (0.0082)  time: 0.8355  data: 0.1940  max mem: 16066
Test:  [150/213]  eta: 0:00:57  class_error: 64.44  loss: 14.0365 (14.4800)  loss_ce: 10.3462 (11.0230)  loss_bbox: 2.1698 (2.1425)  loss_giou: 1.2119 (1.2321)  loss_conf: 0.0870 (0.0823)  loss_ce_unscaled: 2.0692 (2.2046)  class_error_unscaled: 59.1837 (62.1564)  loss_bbox_unscaled: 0.4340 (0.4285)  loss_giou_unscaled: 0.6059 (0.6161)  loss_conf_unscaled: 0.0087 (0.0082)  time: 0.8752  data: 0.2216  max mem: 16066
Test:  [160/213]  eta: 0:00:48  class_error: 54.35  loss: 14.4163 (14.4813)  loss_ce: 10.7761 (11.0278)  loss_bbox: 2.1698 (2.1389)  loss_giou: 1.2354 (1.2321)  loss_conf: 0.0813 (0.0825)  loss_ce_unscaled: 2.1552 (2.2056)  class_error_unscaled: 60.8247 (62.0537)  loss_bbox_unscaled: 0.4340 (0.4278)  loss_giou_unscaled: 0.6177 (0.6160)  loss_conf_unscaled: 0.0081 (0.0083)  time: 0.8405  data: 0.2030  max mem: 16066
Test:  [170/213]  eta: 0:00:38  class_error: 54.02  loss: 14.4287 (14.4471)  loss_ce: 10.7964 (11.0014)  loss_bbox: 2.1250 (2.1340)  loss_giou: 1.2002 (1.2293)  loss_conf: 0.0802 (0.0824)  loss_ce_unscaled: 2.1593 (2.2003)  class_error_unscaled: 60.8247 (61.8770)  loss_bbox_unscaled: 0.4250 (0.4268)  loss_giou_unscaled: 0.6001 (0.6146)  loss_conf_unscaled: 0.0080 (0.0082)  time: 0.8013  data: 0.1735  max mem: 16066
Test:  [180/213]  eta: 0:00:29  class_error: 61.00  loss: 14.2587 (14.4555)  loss_ce: 10.9205 (11.0110)  loss_bbox: 2.0539 (2.1314)  loss_giou: 1.2166 (1.2304)  loss_conf: 0.0736 (0.0828)  loss_ce_unscaled: 2.1841 (2.2022)  class_error_unscaled: 61.2245 (61.9910)  loss_bbox_unscaled: 0.4108 (0.4263)  loss_giou_unscaled: 0.6083 (0.6152)  loss_conf_unscaled: 0.0074 (0.0083)  time: 0.8398  data: 0.1914  max mem: 16066
Test:  [190/213]  eta: 0:00:20  class_error: 67.39  loss: 14.3665 (14.4586)  loss_ce: 11.2205 (11.0155)  loss_bbox: 2.0272 (2.1301)  loss_giou: 1.2347 (1.2299)  loss_conf: 0.0859 (0.0831)  loss_ce_unscaled: 2.2441 (2.2031)  class_error_unscaled: 63.0000 (62.0594)  loss_bbox_unscaled: 0.4054 (0.4260)  loss_giou_unscaled: 0.6173 (0.6150)  loss_conf_unscaled: 0.0086 (0.0083)  time: 0.8772  data: 0.2189  max mem: 16066
Test:  [200/213]  eta: 0:00:11  class_error: 69.23  loss: 14.1398 (14.4487)  loss_ce: 10.8945 (11.0070)  loss_bbox: 2.0749 (2.1289)  loss_giou: 1.2090 (1.2293)  loss_conf: 0.0813 (0.0834)  loss_ce_unscaled: 2.1789 (2.2014)  class_error_unscaled: 63.2075 (62.1264)  loss_bbox_unscaled: 0.4150 (0.4258)  loss_giou_unscaled: 0.6045 (0.6147)  loss_conf_unscaled: 0.0081 (0.0083)  time: 0.7475  data: 0.1997  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 59.09  loss: 14.3367 (14.4738)  loss_ce: 10.8945 (11.0376)  loss_bbox: 2.0749 (2.1242)  loss_giou: 1.2090 (1.2289)  loss_conf: 0.0775 (0.0831)  loss_ce_unscaled: 2.1789 (2.2075)  class_error_unscaled: 63.2075 (62.1534)  loss_bbox_unscaled: 0.4150 (0.4248)  loss_giou_unscaled: 0.6045 (0.6144)  loss_conf_unscaled: 0.0078 (0.0083)  time: 0.8792  data: 0.3289  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 62.96  loss: 14.2994 (14.4792)  loss_ce: 10.8945 (11.0437)  loss_bbox: 2.0192 (2.1234)  loss_giou: 1.2090 (1.2291)  loss_conf: 0.0775 (0.0830)  loss_ce_unscaled: 2.1789 (2.2087)  class_error_unscaled: 63.2075 (62.2137)  loss_bbox_unscaled: 0.4038 (0.4247)  loss_giou_unscaled: 0.6045 (0.6146)  loss_conf_unscaled: 0.0078 (0.0083)  time: 0.8501  data: 0.3236  max mem: 16066
Test: Total time: 0:03:10 (0.8935 s / it)
Averaged stats: class_error: 62.96  loss: 14.2994 (14.4792)  loss_ce: 10.8945 (11.0437)  loss_bbox: 2.0192 (2.1234)  loss_giou: 1.2090 (1.2291)  loss_conf: 0.0775 (0.0830)  loss_ce_unscaled: 2.1789 (2.2087)  class_error_unscaled: 63.2075 (62.2137)  loss_bbox_unscaled: 0.4038 (0.4247)  loss_giou_unscaled: 0.6045 (0.6146)  loss_conf_unscaled: 0.0078 (0.0083)
zero-shot mAP: 11.84
rare mAP: 16.93
nonrare mAP: 24.46
full mAP: 17.43

================== checkpoint0115.pth 结束：2025-09-11 09:23:43.449297 ==================

================== checkpoint0110.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0110.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:20:56  class_error: 60.00  loss: 13.8749 (13.8749)  loss_ce: 10.3843 (10.3843)  loss_bbox: 2.1406 (2.1406)  loss_giou: 1.2609 (1.2609)  loss_conf: 0.0891 (0.0891)  loss_ce_unscaled: 2.0769 (2.0769)  class_error_unscaled: 60.0000 (60.0000)  loss_bbox_unscaled: 0.4281 (0.4281)  loss_giou_unscaled: 0.6305 (0.6305)  loss_conf_unscaled: 0.0089 (0.0089)  time: 5.8991  data: 2.7418  max mem: 16066
Test:  [ 10/213]  eta: 0:04:46  class_error: 50.00  loss: 13.8749 (14.4981)  loss_ce: 10.5939 (10.9042)  loss_bbox: 2.2551 (2.2266)  loss_giou: 1.2753 (1.2784)  loss_conf: 0.0891 (0.0889)  loss_ce_unscaled: 2.1188 (2.1808)  class_error_unscaled: 62.5000 (61.4152)  loss_bbox_unscaled: 0.4510 (0.4453)  loss_giou_unscaled: 0.6377 (0.6392)  loss_conf_unscaled: 0.0089 (0.0089)  time: 1.4113  data: 0.5668  max mem: 16066
Test:  [ 20/213]  eta: 0:04:21  class_error: 72.16  loss: 14.5145 (14.4873)  loss_ce: 10.9146 (10.9888)  loss_bbox: 2.1405 (2.1670)  loss_giou: 1.2436 (1.2395)  loss_conf: 0.0891 (0.0920)  loss_ce_unscaled: 2.1829 (2.1978)  class_error_unscaled: 63.3333 (62.4990)  loss_bbox_unscaled: 0.4281 (0.4334)  loss_giou_unscaled: 0.6218 (0.6198)  loss_conf_unscaled: 0.0089 (0.0092)  time: 1.1252  data: 0.4246  max mem: 16066
Test:  [ 30/213]  eta: 0:03:52  class_error: 61.36  loss: 14.6742 (14.4138)  loss_ce: 11.2243 (10.9734)  loss_bbox: 2.0722 (2.1371)  loss_giou: 1.1808 (1.2164)  loss_conf: 0.0772 (0.0868)  loss_ce_unscaled: 2.2449 (2.1947)  class_error_unscaled: 64.2857 (62.8060)  loss_bbox_unscaled: 0.4144 (0.4274)  loss_giou_unscaled: 0.5904 (0.6082)  loss_conf_unscaled: 0.0077 (0.0087)  time: 1.1942  data: 0.4409  max mem: 16066
Test:  [ 40/213]  eta: 0:03:22  class_error: 59.57  loss: 14.0899 (14.3004)  loss_ce: 10.6130 (10.8767)  loss_bbox: 2.1107 (2.1296)  loss_giou: 1.1808 (1.2102)  loss_conf: 0.0717 (0.0839)  loss_ce_unscaled: 2.1226 (2.1753)  class_error_unscaled: 64.0449 (63.0965)  loss_bbox_unscaled: 0.4221 (0.4259)  loss_giou_unscaled: 0.5904 (0.6051)  loss_conf_unscaled: 0.0072 (0.0084)  time: 0.9851  data: 0.2944  max mem: 16066
Test:  [ 50/213]  eta: 0:03:02  class_error: 58.51  loss: 13.7407 (14.2772)  loss_ce: 10.4727 (10.8607)  loss_bbox: 2.0639 (2.1231)  loss_giou: 1.1940 (1.2098)  loss_conf: 0.0783 (0.0837)  loss_ce_unscaled: 2.0945 (2.1721)  class_error_unscaled: 60.9195 (62.4578)  loss_bbox_unscaled: 0.4128 (0.4246)  loss_giou_unscaled: 0.5970 (0.6049)  loss_conf_unscaled: 0.0078 (0.0084)  time: 0.8874  data: 0.2475  max mem: 16066
Test:  [ 60/213]  eta: 0:02:41  class_error: 59.14  loss: 13.5568 (14.2241)  loss_ce: 10.3794 (10.8248)  loss_bbox: 1.9712 (2.1070)  loss_giou: 1.2095 (1.2091)  loss_conf: 0.0788 (0.0832)  loss_ce_unscaled: 2.0759 (2.1650)  class_error_unscaled: 58.3333 (61.9782)  loss_bbox_unscaled: 0.3942 (0.4214)  loss_giou_unscaled: 0.6047 (0.6045)  loss_conf_unscaled: 0.0079 (0.0083)  time: 0.8123  data: 0.2350  max mem: 16066
Test:  [ 70/213]  eta: 0:02:25  class_error: 58.43  loss: 13.6136 (14.2497)  loss_ce: 10.4560 (10.8547)  loss_bbox: 1.9712 (2.1044)  loss_giou: 1.2101 (1.2064)  loss_conf: 0.0846 (0.0842)  loss_ce_unscaled: 2.0912 (2.1709)  class_error_unscaled: 59.1398 (61.8267)  loss_bbox_unscaled: 0.3942 (0.4209)  loss_giou_unscaled: 0.6051 (0.6032)  loss_conf_unscaled: 0.0085 (0.0084)  time: 0.7495  data: 0.1765  max mem: 16066
Test:  [ 80/213]  eta: 0:02:11  class_error: 51.06  loss: 14.1393 (14.2785)  loss_ce: 10.8600 (10.8835)  loss_bbox: 2.0761 (2.1016)  loss_giou: 1.2101 (1.2089)  loss_conf: 0.0841 (0.0844)  loss_ce_unscaled: 2.1720 (2.1767)  class_error_unscaled: 60.8696 (62.0823)  loss_bbox_unscaled: 0.4152 (0.4203)  loss_giou_unscaled: 0.6051 (0.6045)  loss_conf_unscaled: 0.0084 (0.0084)  time: 0.7800  data: 0.1709  max mem: 16066
Test:  [ 90/213]  eta: 0:01:58  class_error: 55.56  loss: 14.7374 (14.3313)  loss_ce: 11.0292 (10.9133)  loss_bbox: 2.1404 (2.1182)  loss_giou: 1.2577 (1.2153)  loss_conf: 0.0733 (0.0844)  loss_ce_unscaled: 2.2058 (2.1827)  class_error_unscaled: 62.2642 (62.2393)  loss_bbox_unscaled: 0.4281 (0.4236)  loss_giou_unscaled: 0.6288 (0.6077)  loss_conf_unscaled: 0.0073 (0.0084)  time: 0.7870  data: 0.1707  max mem: 16066
Test:  [100/213]  eta: 0:01:46  class_error: 60.22  loss: 14.7374 (14.3882)  loss_ce: 11.3552 (10.9563)  loss_bbox: 2.2492 (2.1279)  loss_giou: 1.2577 (1.2189)  loss_conf: 0.0813 (0.0850)  loss_ce_unscaled: 2.2710 (2.1913)  class_error_unscaled: 62.2642 (62.3337)  loss_bbox_unscaled: 0.4498 (0.4256)  loss_giou_unscaled: 0.6288 (0.6095)  loss_conf_unscaled: 0.0081 (0.0085)  time: 0.7440  data: 0.1690  max mem: 16066
Test:  [110/213]  eta: 0:01:35  class_error: 61.96  loss: 14.1724 (14.3437)  loss_ce: 10.4945 (10.9047)  loss_bbox: 2.1983 (2.1327)  loss_giou: 1.2575 (1.2224)  loss_conf: 0.0833 (0.0838)  loss_ce_unscaled: 2.0989 (2.1809)  class_error_unscaled: 61.9565 (62.1883)  loss_bbox_unscaled: 0.4397 (0.4265)  loss_giou_unscaled: 0.6287 (0.6112)  loss_conf_unscaled: 0.0083 (0.0084)  time: 0.7691  data: 0.2372  max mem: 16066
Test:  [120/213]  eta: 0:01:25  class_error: 67.44  loss: 14.3434 (14.3833)  loss_ce: 10.8037 (10.9312)  loss_bbox: 2.1407 (2.1417)  loss_giou: 1.2801 (1.2281)  loss_conf: 0.0652 (0.0823)  loss_ce_unscaled: 2.1607 (2.1862)  class_error_unscaled: 62.5000 (62.3683)  loss_bbox_unscaled: 0.4281 (0.4283)  loss_giou_unscaled: 0.6400 (0.6140)  loss_conf_unscaled: 0.0065 (0.0082)  time: 0.8463  data: 0.2392  max mem: 16066
Test:  [130/213]  eta: 0:01:15  class_error: 62.37  loss: 14.7126 (14.4098)  loss_ce: 10.9929 (10.9478)  loss_bbox: 2.2550 (2.1476)  loss_giou: 1.2952 (1.2330)  loss_conf: 0.0652 (0.0814)  loss_ce_unscaled: 2.1986 (2.1896)  class_error_unscaled: 64.4444 (62.6907)  loss_bbox_unscaled: 0.4510 (0.4295)  loss_giou_unscaled: 0.6476 (0.6165)  loss_conf_unscaled: 0.0065 (0.0081)  time: 0.8092  data: 0.1901  max mem: 16066
Test:  [140/213]  eta: 0:01:06  class_error: 62.89  loss: 14.3840 (14.3931)  loss_ce: 10.8021 (10.9377)  loss_bbox: 2.1254 (2.1435)  loss_giou: 1.2125 (1.2300)  loss_conf: 0.0763 (0.0819)  loss_ce_unscaled: 2.1604 (2.1875)  class_error_unscaled: 62.8866 (62.3482)  loss_bbox_unscaled: 0.4251 (0.4287)  loss_giou_unscaled: 0.6062 (0.6150)  loss_conf_unscaled: 0.0076 (0.0082)  time: 0.8293  data: 0.2062  max mem: 16066
Test:  [150/213]  eta: 0:00:56  class_error: 65.56  loss: 13.7834 (14.3771)  loss_ce: 10.2308 (10.9180)  loss_bbox: 2.1254 (2.1446)  loss_giou: 1.2110 (1.2318)  loss_conf: 0.0827 (0.0827)  loss_ce_unscaled: 2.0462 (2.1836)  class_error_unscaled: 60.6383 (62.2529)  loss_bbox_unscaled: 0.4251 (0.4289)  loss_giou_unscaled: 0.6055 (0.6159)  loss_conf_unscaled: 0.0083 (0.0083)  time: 0.8332  data: 0.2188  max mem: 16066
Test:  [160/213]  eta: 0:00:47  class_error: 54.35  loss: 14.2705 (14.3755)  loss_ce: 10.7029 (10.9193)  loss_bbox: 2.1431 (2.1416)  loss_giou: 1.2116 (1.2317)  loss_conf: 0.0815 (0.0829)  loss_ce_unscaled: 2.1406 (2.1839)  class_error_unscaled: 61.9048 (62.1843)  loss_bbox_unscaled: 0.4286 (0.4283)  loss_giou_unscaled: 0.6058 (0.6159)  loss_conf_unscaled: 0.0082 (0.0083)  time: 0.8569  data: 0.2484  max mem: 16066
Test:  [170/213]  eta: 0:00:38  class_error: 55.17  loss: 14.2873 (14.3425)  loss_ce: 10.7029 (10.8946)  loss_bbox: 2.1101 (2.1362)  loss_giou: 1.1982 (1.2289)  loss_conf: 0.0799 (0.0829)  loss_ce_unscaled: 2.1406 (2.1789)  class_error_unscaled: 60.8247 (62.0138)  loss_bbox_unscaled: 0.4220 (0.4272)  loss_giou_unscaled: 0.5991 (0.6144)  loss_conf_unscaled: 0.0080 (0.0083)  time: 0.8532  data: 0.2425  max mem: 16066
Test:  [180/213]  eta: 0:00:29  class_error: 63.00  loss: 13.9634 (14.3459)  loss_ce: 10.8201 (10.8974)  loss_bbox: 2.0645 (2.1350)  loss_giou: 1.2144 (1.2304)  loss_conf: 0.0799 (0.0830)  loss_ce_unscaled: 2.1640 (2.1795)  class_error_unscaled: 61.5385 (62.1177)  loss_bbox_unscaled: 0.4129 (0.4270)  loss_giou_unscaled: 0.6072 (0.6152)  loss_conf_unscaled: 0.0080 (0.0083)  time: 0.8579  data: 0.2252  max mem: 16066
Test:  [190/213]  eta: 0:00:20  class_error: 69.57  loss: 14.2366 (14.3464)  loss_ce: 10.9316 (10.8987)  loss_bbox: 2.0528 (2.1346)  loss_giou: 1.2396 (1.2299)  loss_conf: 0.0854 (0.0832)  loss_ce_unscaled: 2.1863 (2.1797)  class_error_unscaled: 63.7363 (62.1623)  loss_bbox_unscaled: 0.4106 (0.4269)  loss_giou_unscaled: 0.6198 (0.6149)  loss_conf_unscaled: 0.0085 (0.0083)  time: 0.8523  data: 0.2191  max mem: 16066
Test:  [200/213]  eta: 0:00:11  class_error: 70.33  loss: 14.0678 (14.3421)  loss_ce: 10.8344 (10.8959)  loss_bbox: 2.0528 (2.1332)  loss_giou: 1.2043 (1.2295)  loss_conf: 0.0819 (0.0835)  loss_ce_unscaled: 2.1669 (2.1792)  class_error_unscaled: 63.7363 (62.2350)  loss_bbox_unscaled: 0.4106 (0.4266)  loss_giou_unscaled: 0.6021 (0.6147)  loss_conf_unscaled: 0.0082 (0.0084)  time: 0.9202  data: 0.2626  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 61.36  loss: 14.2304 (14.3660)  loss_ce: 10.8344 (10.9264)  loss_bbox: 2.0429 (2.1277)  loss_giou: 1.2080 (1.2285)  loss_conf: 0.0812 (0.0835)  loss_ce_unscaled: 2.1669 (2.1853)  class_error_unscaled: 63.2075 (62.2399)  loss_bbox_unscaled: 0.4086 (0.4255)  loss_giou_unscaled: 0.6040 (0.6142)  loss_conf_unscaled: 0.0081 (0.0083)  time: 0.9396  data: 0.2792  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 62.96  loss: 14.0678 (14.3708)  loss_ce: 10.7783 (10.9319)  loss_bbox: 2.0017 (2.1267)  loss_giou: 1.2080 (1.2288)  loss_conf: 0.0812 (0.0834)  loss_ce_unscaled: 2.1557 (2.1864)  class_error_unscaled: 63.2075 (62.2994)  loss_bbox_unscaled: 0.4003 (0.4253)  loss_giou_unscaled: 0.6040 (0.6144)  loss_conf_unscaled: 0.0081 (0.0083)  time: 0.9165  data: 0.2710  max mem: 16066
Test: Total time: 0:03:10 (0.8944 s / it)
Averaged stats: class_error: 62.96  loss: 14.0678 (14.3708)  loss_ce: 10.7783 (10.9319)  loss_bbox: 2.0017 (2.1267)  loss_giou: 1.2080 (1.2288)  loss_conf: 0.0812 (0.0834)  loss_ce_unscaled: 2.1557 (2.1864)  class_error_unscaled: 63.2075 (62.2994)  loss_bbox_unscaled: 0.4003 (0.4253)  loss_giou_unscaled: 0.6040 (0.6144)  loss_conf_unscaled: 0.0081 (0.0083)
zero-shot mAP: 11.86
rare mAP: 17.07
nonrare mAP: 24.33
full mAP: 17.47

================== checkpoint0110.pth 结束：2025-09-11 09:27:45.085910 ==================

================== checkpoint0105.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0105.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:18:13  class_error: 55.56  loss: 14.4241 (14.4241)  loss_ce: 11.0037 (11.0037)  loss_bbox: 2.0849 (2.0849)  loss_giou: 1.2441 (1.2441)  loss_conf: 0.0914 (0.0914)  loss_ce_unscaled: 2.2007 (2.2007)  class_error_unscaled: 55.5556 (55.5556)  loss_bbox_unscaled: 0.4170 (0.4170)  loss_giou_unscaled: 0.6220 (0.6220)  loss_conf_unscaled: 0.0091 (0.0091)  time: 5.1341  data: 2.0396  max mem: 16066
Test:  [ 10/213]  eta: 0:03:44  class_error: 52.13  loss: 14.7818 (15.2019)  loss_ce: 11.2677 (11.6141)  loss_bbox: 2.2435 (2.2149)  loss_giou: 1.2839 (1.2850)  loss_conf: 0.0846 (0.0880)  loss_ce_unscaled: 2.2535 (2.3228)  class_error_unscaled: 63.3333 (61.8010)  loss_bbox_unscaled: 0.4487 (0.4430)  loss_giou_unscaled: 0.6420 (0.6425)  loss_conf_unscaled: 0.0085 (0.0088)  time: 1.1048  data: 0.3282  max mem: 16066
Test:  [ 20/213]  eta: 0:02:57  class_error: 72.16  loss: 14.9738 (15.1226)  loss_ce: 11.6062 (11.6329)  loss_bbox: 2.1560 (2.1570)  loss_giou: 1.2451 (1.2430)  loss_conf: 0.0846 (0.0896)  loss_ce_unscaled: 2.3212 (2.3266)  class_error_unscaled: 63.3333 (62.1086)  loss_bbox_unscaled: 0.4312 (0.4314)  loss_giou_unscaled: 0.6225 (0.6215)  loss_conf_unscaled: 0.0085 (0.0090)  time: 0.7094  data: 0.1510  max mem: 16066
Test:  [ 30/213]  eta: 0:02:36  class_error: 60.23  loss: 15.2382 (15.0279)  loss_ce: 11.6146 (11.5913)  loss_bbox: 2.0975 (2.1303)  loss_giou: 1.1817 (1.2203)  loss_conf: 0.0803 (0.0860)  loss_ce_unscaled: 2.3229 (2.3183)  class_error_unscaled: 62.7907 (62.3288)  loss_bbox_unscaled: 0.4195 (0.4261)  loss_giou_unscaled: 0.5909 (0.6101)  loss_conf_unscaled: 0.0080 (0.0086)  time: 0.7169  data: 0.1836  max mem: 16066
Test:  [ 40/213]  eta: 0:02:25  class_error: 60.64  loss: 14.8152 (14.9141)  loss_ce: 11.2558 (11.4927)  loss_bbox: 2.1036 (2.1242)  loss_giou: 1.2108 (1.2141)  loss_conf: 0.0792 (0.0832)  loss_ce_unscaled: 2.2512 (2.2985)  class_error_unscaled: 63.2653 (62.7125)  loss_bbox_unscaled: 0.4207 (0.4248)  loss_giou_unscaled: 0.6054 (0.6070)  loss_conf_unscaled: 0.0079 (0.0083)  time: 0.7584  data: 0.1927  max mem: 16066
Test:  [ 50/213]  eta: 0:02:14  class_error: 58.51  loss: 14.6421 (14.9067)  loss_ce: 11.0100 (11.4904)  loss_bbox: 2.0877 (2.1185)  loss_giou: 1.2172 (1.2147)  loss_conf: 0.0791 (0.0831)  loss_ce_unscaled: 2.2020 (2.2981)  class_error_unscaled: 62.0690 (62.2400)  loss_bbox_unscaled: 0.4175 (0.4237)  loss_giou_unscaled: 0.6086 (0.6074)  loss_conf_unscaled: 0.0079 (0.0083)  time: 0.7814  data: 0.1723  max mem: 16066
Test:  [ 60/213]  eta: 0:02:03  class_error: 58.06  loss: 14.2971 (14.8576)  loss_ce: 11.1191 (11.4605)  loss_bbox: 1.9917 (2.1009)  loss_giou: 1.2280 (1.2132)  loss_conf: 0.0790 (0.0830)  loss_ce_unscaled: 2.2238 (2.2921)  class_error_unscaled: 58.0645 (61.7748)  loss_bbox_unscaled: 0.3983 (0.4202)  loss_giou_unscaled: 0.6140 (0.6066)  loss_conf_unscaled: 0.0079 (0.0083)  time: 0.7404  data: 0.1683  max mem: 16066
Test:  [ 70/213]  eta: 0:01:52  class_error: 61.80  loss: 14.2426 (14.8981)  loss_ce: 11.2345 (11.5026)  loss_bbox: 2.0160 (2.1005)  loss_giou: 1.2215 (1.2111)  loss_conf: 0.0851 (0.0840)  loss_ce_unscaled: 2.2469 (2.3005)  class_error_unscaled: 58.6957 (61.6618)  loss_bbox_unscaled: 0.4032 (0.4201)  loss_giou_unscaled: 0.6108 (0.6055)  loss_conf_unscaled: 0.0085 (0.0084)  time: 0.6952  data: 0.1553  max mem: 16066
Test:  [ 80/213]  eta: 0:01:44  class_error: 52.13  loss: 14.9524 (14.9410)  loss_ce: 11.5409 (11.5462)  loss_bbox: 2.0256 (2.0970)  loss_giou: 1.2186 (1.2126)  loss_conf: 0.0861 (0.0853)  loss_ce_unscaled: 2.3082 (2.3092)  class_error_unscaled: 60.4651 (61.8602)  loss_bbox_unscaled: 0.4051 (0.4194)  loss_giou_unscaled: 0.6093 (0.6063)  loss_conf_unscaled: 0.0086 (0.0085)  time: 0.7078  data: 0.1569  max mem: 16066
Test:  [ 90/213]  eta: 0:01:36  class_error: 56.67  loss: 15.5667 (14.9992)  loss_ce: 12.0165 (11.5832)  loss_bbox: 2.1921 (2.1121)  loss_giou: 1.2502 (1.2180)  loss_conf: 0.0740 (0.0860)  loss_ce_unscaled: 2.4033 (2.3166)  class_error_unscaled: 62.2642 (62.0088)  loss_bbox_unscaled: 0.4384 (0.4224)  loss_giou_unscaled: 0.6251 (0.6090)  loss_conf_unscaled: 0.0074 (0.0086)  time: 0.7861  data: 0.1906  max mem: 16066
Test:  [100/213]  eta: 0:01:29  class_error: 61.29  loss: 15.5667 (15.0743)  loss_ce: 12.0330 (11.6447)  loss_bbox: 2.2171 (2.1218)  loss_giou: 1.2504 (1.2213)  loss_conf: 0.0850 (0.0865)  loss_ce_unscaled: 2.4066 (2.3289)  class_error_unscaled: 62.2642 (62.1500)  loss_bbox_unscaled: 0.4434 (0.4244)  loss_giou_unscaled: 0.6252 (0.6106)  loss_conf_unscaled: 0.0085 (0.0087)  time: 0.8138  data: 0.1886  max mem: 16066
Test:  [110/213]  eta: 0:01:20  class_error: 64.13  loss: 15.0358 (15.0374)  loss_ce: 11.3079 (11.6005)  loss_bbox: 2.1624 (2.1267)  loss_giou: 1.2608 (1.2250)  loss_conf: 0.0726 (0.0852)  loss_ce_unscaled: 2.2616 (2.3201)  class_error_unscaled: 63.5294 (62.0374)  loss_bbox_unscaled: 0.4325 (0.4253)  loss_giou_unscaled: 0.6304 (0.6125)  loss_conf_unscaled: 0.0073 (0.0085)  time: 0.7370  data: 0.1555  max mem: 16066
Test:  [120/213]  eta: 0:01:12  class_error: 67.44  loss: 14.9490 (15.0783)  loss_ce: 11.5110 (11.6304)  loss_bbox: 2.1368 (2.1340)  loss_giou: 1.2849 (1.2302)  loss_conf: 0.0670 (0.0837)  loss_ce_unscaled: 2.3022 (2.3261)  class_error_unscaled: 63.6364 (62.2622)  loss_bbox_unscaled: 0.4274 (0.4268)  loss_giou_unscaled: 0.6425 (0.6151)  loss_conf_unscaled: 0.0067 (0.0084)  time: 0.7326  data: 0.1614  max mem: 16066
Test:  [130/213]  eta: 0:01:04  class_error: 62.37  loss: 15.3717 (15.1054)  loss_ce: 11.8259 (11.6472)  loss_bbox: 2.1352 (2.1400)  loss_giou: 1.2911 (1.2354)  loss_conf: 0.0666 (0.0827)  loss_ce_unscaled: 2.3652 (2.3294)  class_error_unscaled: 64.7059 (62.6347)  loss_bbox_unscaled: 0.4270 (0.4280)  loss_giou_unscaled: 0.6456 (0.6177)  loss_conf_unscaled: 0.0067 (0.0083)  time: 0.7334  data: 0.1629  max mem: 16066
Test:  [140/213]  eta: 0:00:56  class_error: 63.92  loss: 14.7625 (15.0888)  loss_ce: 11.5496 (11.6381)  loss_bbox: 2.1352 (2.1354)  loss_giou: 1.2182 (1.2321)  loss_conf: 0.0767 (0.0831)  loss_ce_unscaled: 2.3099 (2.3276)  class_error_unscaled: 63.3333 (62.2942)  loss_bbox_unscaled: 0.4270 (0.4271)  loss_giou_unscaled: 0.6091 (0.6161)  loss_conf_unscaled: 0.0077 (0.0083)  time: 0.7370  data: 0.1622  max mem: 16066
Test:  [150/213]  eta: 0:00:48  class_error: 64.44  loss: 14.4060 (15.0765)  loss_ce: 10.9561 (11.6211)  loss_bbox: 2.1102 (2.1377)  loss_giou: 1.2092 (1.2340)  loss_conf: 0.0817 (0.0838)  loss_ce_unscaled: 2.1912 (2.3242)  class_error_unscaled: 59.1837 (62.2244)  loss_bbox_unscaled: 0.4220 (0.4275)  loss_giou_unscaled: 0.6046 (0.6170)  loss_conf_unscaled: 0.0082 (0.0084)  time: 0.7434  data: 0.1640  max mem: 16066
Test:  [160/213]  eta: 0:00:40  class_error: 53.26  loss: 14.9037 (15.0596)  loss_ce: 11.1900 (11.6096)  loss_bbox: 2.0844 (2.1324)  loss_giou: 1.2092 (1.2333)  loss_conf: 0.0820 (0.0843)  loss_ce_unscaled: 2.2380 (2.3219)  class_error_unscaled: 61.8557 (62.1034)  loss_bbox_unscaled: 0.4169 (0.4265)  loss_giou_unscaled: 0.6046 (0.6167)  loss_conf_unscaled: 0.0082 (0.0084)  time: 0.7399  data: 0.1571  max mem: 16066
Test:  [170/213]  eta: 0:00:32  class_error: 51.72  loss: 14.4317 (15.0248)  loss_ce: 11.3571 (11.5824)  loss_bbox: 2.0257 (2.1275)  loss_giou: 1.2060 (1.2308)  loss_conf: 0.0751 (0.0841)  loss_ce_unscaled: 2.2714 (2.3165)  class_error_unscaled: 60.2041 (61.8630)  loss_bbox_unscaled: 0.4051 (0.4255)  loss_giou_unscaled: 0.6030 (0.6154)  loss_conf_unscaled: 0.0075 (0.0084)  time: 0.7302  data: 0.1550  max mem: 16066
Test:  [180/213]  eta: 0:00:25  class_error: 62.00  loss: 14.6160 (15.0295)  loss_ce: 11.5255 (11.5882)  loss_bbox: 2.0388 (2.1248)  loss_giou: 1.2201 (1.2318)  loss_conf: 0.0737 (0.0846)  loss_ce_unscaled: 2.3051 (2.3176)  class_error_unscaled: 61.9565 (62.0121)  loss_bbox_unscaled: 0.4078 (0.4250)  loss_giou_unscaled: 0.6101 (0.6159)  loss_conf_unscaled: 0.0074 (0.0085)  time: 0.7465  data: 0.1598  max mem: 16066
Test:  [190/213]  eta: 0:00:17  class_error: 68.48  loss: 15.1496 (15.0383)  loss_ce: 11.8274 (11.5975)  loss_bbox: 2.0427 (2.1243)  loss_giou: 1.2238 (1.2313)  loss_conf: 0.0939 (0.0851)  loss_ce_unscaled: 2.3655 (2.3195)  class_error_unscaled: 64.9485 (62.0482)  loss_bbox_unscaled: 0.4085 (0.4249)  loss_giou_unscaled: 0.6119 (0.6157)  loss_conf_unscaled: 0.0094 (0.0085)  time: 0.7472  data: 0.1595  max mem: 16066
Test:  [200/213]  eta: 0:00:09  class_error: 70.33  loss: 14.6750 (15.0333)  loss_ce: 11.5626 (11.5934)  loss_bbox: 2.0747 (2.1237)  loss_giou: 1.2088 (1.2309)  loss_conf: 0.0891 (0.0853)  loss_ce_unscaled: 2.3125 (2.3187)  class_error_unscaled: 62.8866 (62.1016)  loss_bbox_unscaled: 0.4149 (0.4247)  loss_giou_unscaled: 0.6044 (0.6155)  loss_conf_unscaled: 0.0089 (0.0085)  time: 0.7616  data: 0.1571  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 61.36  loss: 14.8179 (15.0635)  loss_ce: 11.5626 (11.6297)  loss_bbox: 2.0233 (2.1184)  loss_giou: 1.2088 (1.2304)  loss_conf: 0.0807 (0.0850)  loss_ce_unscaled: 2.3125 (2.3259)  class_error_unscaled: 62.2449 (62.1615)  loss_bbox_unscaled: 0.4047 (0.4237)  loss_giou_unscaled: 0.6044 (0.6152)  loss_conf_unscaled: 0.0081 (0.0085)  time: 0.7714  data: 0.1634  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 66.67  loss: 14.8179 (15.0705)  loss_ce: 11.5626 (11.6373)  loss_bbox: 2.0027 (2.1177)  loss_giou: 1.2051 (1.2307)  loss_conf: 0.0845 (0.0849)  loss_ce_unscaled: 2.3125 (2.3275)  class_error_unscaled: 62.2449 (62.2290)  loss_bbox_unscaled: 0.4005 (0.4235)  loss_giou_unscaled: 0.6026 (0.6153)  loss_conf_unscaled: 0.0085 (0.0085)  time: 0.7527  data: 0.1595  max mem: 16066
Test: Total time: 0:02:42 (0.7617 s / it)
Averaged stats: class_error: 66.67  loss: 14.8179 (15.0705)  loss_ce: 11.5626 (11.6373)  loss_bbox: 2.0027 (2.1177)  loss_giou: 1.2051 (1.2307)  loss_conf: 0.0845 (0.0849)  loss_ce_unscaled: 2.3125 (2.3275)  class_error_unscaled: 62.2449 (62.2290)  loss_bbox_unscaled: 0.4005 (0.4235)  loss_giou_unscaled: 0.6026 (0.6153)  loss_conf_unscaled: 0.0085 (0.0085)
zero-shot mAP: 11.93
rare mAP: 17.19
nonrare mAP: 24.37
full mAP: 17.57

================== checkpoint0105.pth 结束：2025-09-11 09:31:11.648549 ==================

================== checkpoint0100.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0100.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:17:22  class_error: 55.56  loss: 14.0447 (14.0447)  loss_ce: 10.5479 (10.5479)  loss_bbox: 2.1352 (2.1352)  loss_giou: 1.2840 (1.2840)  loss_conf: 0.0776 (0.0776)  loss_ce_unscaled: 2.1096 (2.1096)  class_error_unscaled: 55.5556 (55.5556)  loss_bbox_unscaled: 0.4270 (0.4270)  loss_giou_unscaled: 0.6420 (0.6420)  loss_conf_unscaled: 0.0078 (0.0078)  time: 4.8949  data: 2.5036  max mem: 16066
Test:  [ 10/213]  eta: 0:03:44  class_error: 51.06  loss: 14.7085 (15.1043)  loss_ce: 11.4728 (11.5540)  loss_bbox: 2.2028 (2.1835)  loss_giou: 1.2784 (1.2758)  loss_conf: 0.0776 (0.0910)  loss_ce_unscaled: 2.2946 (2.3108)  class_error_unscaled: 61.2500 (61.0815)  loss_bbox_unscaled: 0.4406 (0.4367)  loss_giou_unscaled: 0.6392 (0.6379)  loss_conf_unscaled: 0.0078 (0.0091)  time: 1.1070  data: 0.4174  max mem: 16066
Test:  [ 20/213]  eta: 0:03:02  class_error: 74.23  loss: 15.0131 (15.1014)  loss_ce: 11.6047 (11.6235)  loss_bbox: 2.1667 (2.1451)  loss_giou: 1.2508 (1.2407)  loss_conf: 0.0862 (0.0921)  loss_ce_unscaled: 2.3209 (2.3247)  class_error_unscaled: 64.4444 (63.0308)  loss_bbox_unscaled: 0.4333 (0.4290)  loss_giou_unscaled: 0.6254 (0.6204)  loss_conf_unscaled: 0.0086 (0.0092)  time: 0.7467  data: 0.1834  max mem: 16066
Test:  [ 30/213]  eta: 0:02:40  class_error: 56.82  loss: 14.9898 (14.9450)  loss_ce: 11.7272 (11.5211)  loss_bbox: 2.0579 (2.1191)  loss_giou: 1.1737 (1.2177)  loss_conf: 0.0721 (0.0870)  loss_ce_unscaled: 2.3454 (2.3042)  class_error_unscaled: 63.2653 (62.9188)  loss_bbox_unscaled: 0.4116 (0.4238)  loss_giou_unscaled: 0.5869 (0.6089)  loss_conf_unscaled: 0.0072 (0.0087)  time: 0.7492  data: 0.1690  max mem: 16066
Test:  [ 40/213]  eta: 0:02:26  class_error: 58.51  loss: 14.4297 (14.8066)  loss_ce: 11.0739 (11.3985)  loss_bbox: 2.0727 (2.1127)  loss_giou: 1.1781 (1.2112)  loss_conf: 0.0715 (0.0842)  loss_ce_unscaled: 2.2148 (2.2797)  class_error_unscaled: 63.2653 (63.0139)  loss_bbox_unscaled: 0.4145 (0.4225)  loss_giou_unscaled: 0.5890 (0.6056)  loss_conf_unscaled: 0.0072 (0.0084)  time: 0.7453  data: 0.1650  max mem: 16066
Test:  [ 50/213]  eta: 0:02:14  class_error: 60.64  loss: 14.2837 (14.8138)  loss_ce: 11.0961 (11.4102)  loss_bbox: 2.0669 (2.1086)  loss_giou: 1.2133 (1.2117)  loss_conf: 0.0777 (0.0834)  loss_ce_unscaled: 2.2192 (2.2820)  class_error_unscaled: 60.8247 (62.3444)  loss_bbox_unscaled: 0.4134 (0.4217)  loss_giou_unscaled: 0.6066 (0.6059)  loss_conf_unscaled: 0.0078 (0.0083)  time: 0.7489  data: 0.1553  max mem: 16066
Test:  [ 60/213]  eta: 0:02:04  class_error: 60.22  loss: 14.2837 (14.7456)  loss_ce: 11.0961 (11.3602)  loss_bbox: 2.0107 (2.0920)  loss_giou: 1.2174 (1.2101)  loss_conf: 0.0777 (0.0833)  loss_ce_unscaled: 2.2192 (2.2720)  class_error_unscaled: 57.6471 (62.0555)  loss_bbox_unscaled: 0.4021 (0.4184)  loss_giou_unscaled: 0.6087 (0.6050)  loss_conf_unscaled: 0.0078 (0.0083)  time: 0.7368  data: 0.1589  max mem: 16066
Test:  [ 70/213]  eta: 0:01:54  class_error: 57.30  loss: 14.3011 (14.7923)  loss_ce: 11.1480 (11.4063)  loss_bbox: 1.9788 (2.0936)  loss_giou: 1.2173 (1.2093)  loss_conf: 0.0814 (0.0830)  loss_ce_unscaled: 2.2296 (2.2813)  class_error_unscaled: 59.7826 (61.9781)  loss_bbox_unscaled: 0.3958 (0.4187)  loss_giou_unscaled: 0.6087 (0.6047)  loss_conf_unscaled: 0.0081 (0.0083)  time: 0.7423  data: 0.1575  max mem: 16066
Test:  [ 80/213]  eta: 0:01:44  class_error: 54.26  loss: 15.1949 (14.8401)  loss_ce: 11.8122 (11.4505)  loss_bbox: 2.0148 (2.0928)  loss_giou: 1.2067 (1.2130)  loss_conf: 0.0819 (0.0838)  loss_ce_unscaled: 2.3624 (2.2901)  class_error_unscaled: 60.8696 (62.2273)  loss_bbox_unscaled: 0.4030 (0.4186)  loss_giou_unscaled: 0.6033 (0.6065)  loss_conf_unscaled: 0.0082 (0.0084)  time: 0.7090  data: 0.1571  max mem: 16066
Test:  [ 90/213]  eta: 0:01:36  class_error: 55.56  loss: 15.3009 (14.8855)  loss_ce: 11.8122 (11.4755)  loss_bbox: 2.1713 (2.1078)  loss_giou: 1.2491 (1.2185)  loss_conf: 0.0718 (0.0837)  loss_ce_unscaled: 2.3624 (2.2951)  class_error_unscaled: 62.2449 (62.2529)  loss_bbox_unscaled: 0.4343 (0.4216)  loss_giou_unscaled: 0.6246 (0.6093)  loss_conf_unscaled: 0.0072 (0.0084)  time: 0.7076  data: 0.1562  max mem: 16066
Test:  [100/213]  eta: 0:01:28  class_error: 62.37  loss: 15.3553 (14.9679)  loss_ce: 11.9701 (11.5443)  loss_bbox: 2.2332 (2.1178)  loss_giou: 1.2491 (1.2215)  loss_conf: 0.0834 (0.0842)  loss_ce_unscaled: 2.3940 (2.3089)  class_error_unscaled: 62.6506 (62.3352)  loss_bbox_unscaled: 0.4466 (0.4236)  loss_giou_unscaled: 0.6246 (0.6108)  loss_conf_unscaled: 0.0083 (0.0084)  time: 0.7531  data: 0.1539  max mem: 16066
Test:  [110/213]  eta: 0:01:19  class_error: 64.13  loss: 15.0675 (14.9445)  loss_ce: 11.3599 (11.5113)  loss_bbox: 2.1789 (2.1244)  loss_giou: 1.2749 (1.2259)  loss_conf: 0.0834 (0.0828)  loss_ce_unscaled: 2.2720 (2.3023)  class_error_unscaled: 62.6506 (62.1943)  loss_bbox_unscaled: 0.4358 (0.4249)  loss_giou_unscaled: 0.6374 (0.6130)  loss_conf_unscaled: 0.0083 (0.0083)  time: 0.7212  data: 0.1516  max mem: 16066
Test:  [120/213]  eta: 0:01:11  class_error: 66.28  loss: 14.9863 (14.9843)  loss_ce: 11.5175 (11.5374)  loss_bbox: 2.1533 (2.1341)  loss_giou: 1.2819 (1.2314)  loss_conf: 0.0561 (0.0814)  loss_ce_unscaled: 2.3035 (2.3075)  class_error_unscaled: 63.6364 (62.3624)  loss_bbox_unscaled: 0.4307 (0.4268)  loss_giou_unscaled: 0.6410 (0.6157)  loss_conf_unscaled: 0.0056 (0.0081)  time: 0.7361  data: 0.2154  max mem: 16066
Test:  [130/213]  eta: 0:01:04  class_error: 63.44  loss: 15.4071 (15.0104)  loss_ce: 11.6356 (11.5525)  loss_bbox: 2.1728 (2.1406)  loss_giou: 1.2891 (1.2370)  loss_conf: 0.0626 (0.0804)  loss_ce_unscaled: 2.3271 (2.3105)  class_error_unscaled: 63.6364 (62.7012)  loss_bbox_unscaled: 0.4346 (0.4281)  loss_giou_unscaled: 0.6445 (0.6185)  loss_conf_unscaled: 0.0063 (0.0080)  time: 0.7843  data: 0.2732  max mem: 16066
Test:  [140/213]  eta: 0:00:55  class_error: 59.79  loss: 14.8198 (15.0082)  loss_ce: 11.2688 (11.5564)  loss_bbox: 2.1447 (2.1369)  loss_giou: 1.2198 (1.2343)  loss_conf: 0.0709 (0.0806)  loss_ce_unscaled: 2.2538 (2.3113)  class_error_unscaled: 62.9213 (62.3808)  loss_bbox_unscaled: 0.4289 (0.4274)  loss_giou_unscaled: 0.6099 (0.6171)  loss_conf_unscaled: 0.0071 (0.0081)  time: 0.7277  data: 0.2116  max mem: 16066
Test:  [150/213]  eta: 0:00:48  class_error: 66.67  loss: 14.4636 (14.9943)  loss_ce: 10.9809 (11.5382)  loss_bbox: 2.1335 (2.1386)  loss_giou: 1.2096 (1.2359)  loss_conf: 0.0820 (0.0816)  loss_ce_unscaled: 2.1962 (2.3076)  class_error_unscaled: 60.2151 (62.3353)  loss_bbox_unscaled: 0.4267 (0.4277)  loss_giou_unscaled: 0.6048 (0.6179)  loss_conf_unscaled: 0.0082 (0.0082)  time: 0.7345  data: 0.1494  max mem: 16066
Test:  [160/213]  eta: 0:00:40  class_error: 54.35  loss: 14.8386 (14.9937)  loss_ce: 11.4343 (11.5418)  loss_bbox: 2.1482 (2.1346)  loss_giou: 1.2145 (1.2353)  loss_conf: 0.0868 (0.0820)  loss_ce_unscaled: 2.2869 (2.3084)  class_error_unscaled: 61.0000 (62.2027)  loss_bbox_unscaled: 0.4296 (0.4269)  loss_giou_unscaled: 0.6072 (0.6176)  loss_conf_unscaled: 0.0087 (0.0082)  time: 0.7292  data: 0.1473  max mem: 16066
Test:  [170/213]  eta: 0:00:32  class_error: 52.87  loss: 14.6510 (14.9581)  loss_ce: 11.4594 (11.5147)  loss_bbox: 2.1194 (2.1289)  loss_giou: 1.1931 (1.2325)  loss_conf: 0.0781 (0.0820)  loss_ce_unscaled: 2.2919 (2.3029)  class_error_unscaled: 59.7938 (61.9915)  loss_bbox_unscaled: 0.4239 (0.4258)  loss_giou_unscaled: 0.5966 (0.6162)  loss_conf_unscaled: 0.0078 (0.0082)  time: 0.6687  data: 0.1522  max mem: 16066
Test:  [180/213]  eta: 0:00:25  class_error: 64.00  loss: 14.6713 (14.9698)  loss_ce: 11.5352 (11.5254)  loss_bbox: 2.0254 (2.1284)  loss_giou: 1.2269 (1.2341)  loss_conf: 0.0737 (0.0820)  loss_ce_unscaled: 2.3070 (2.3051)  class_error_unscaled: 61.2245 (62.1471)  loss_bbox_unscaled: 0.4051 (0.4257)  loss_giou_unscaled: 0.6135 (0.6170)  loss_conf_unscaled: 0.0074 (0.0082)  time: 0.7426  data: 0.1510  max mem: 16066
Test:  [190/213]  eta: 0:00:17  class_error: 67.39  loss: 15.2089 (14.9689)  loss_ce: 11.6701 (11.5242)  loss_bbox: 2.0254 (2.1282)  loss_giou: 1.2303 (1.2339)  loss_conf: 0.0866 (0.0826)  loss_ce_unscaled: 2.3340 (2.3048)  class_error_unscaled: 64.0000 (62.2117)  loss_bbox_unscaled: 0.4051 (0.4256)  loss_giou_unscaled: 0.6152 (0.6170)  loss_conf_unscaled: 0.0087 (0.0083)  time: 0.7518  data: 0.1542  max mem: 16066
Test:  [200/213]  eta: 0:00:09  class_error: 69.23  loss: 14.6471 (14.9609)  loss_ce: 11.3656 (11.5171)  loss_bbox: 2.0352 (2.1271)  loss_giou: 1.2208 (1.2337)  loss_conf: 0.0836 (0.0830)  loss_ce_unscaled: 2.2731 (2.3034)  class_error_unscaled: 62.6374 (62.2532)  loss_bbox_unscaled: 0.4070 (0.4254)  loss_giou_unscaled: 0.6104 (0.6168)  loss_conf_unscaled: 0.0084 (0.0083)  time: 0.7957  data: 0.1768  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 61.36  loss: 15.0405 (14.9827)  loss_ce: 11.5374 (11.5460)  loss_bbox: 2.0258 (2.1208)  loss_giou: 1.2168 (1.2328)  loss_conf: 0.0803 (0.0830)  loss_ce_unscaled: 2.3075 (2.3092)  class_error_unscaled: 61.3636 (62.3258)  loss_bbox_unscaled: 0.4052 (0.4242)  loss_giou_unscaled: 0.6084 (0.6164)  loss_conf_unscaled: 0.0080 (0.0083)  time: 0.8089  data: 0.1776  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 59.26  loss: 15.0405 (14.9909)  loss_ce: 11.5477 (11.5553)  loss_bbox: 1.9685 (2.1197)  loss_giou: 1.2099 (1.2330)  loss_conf: 0.0817 (0.0830)  loss_ce_unscaled: 2.3095 (2.3111)  class_error_unscaled: 61.3636 (62.3621)  loss_bbox_unscaled: 0.3937 (0.4239)  loss_giou_unscaled: 0.6049 (0.6165)  loss_conf_unscaled: 0.0082 (0.0083)  time: 0.7940  data: 0.1762  max mem: 16066
Test: Total time: 0:02:41 (0.7596 s / it)
Averaged stats: class_error: 59.26  loss: 15.0405 (14.9909)  loss_ce: 11.5477 (11.5553)  loss_bbox: 1.9685 (2.1197)  loss_giou: 1.2099 (1.2330)  loss_conf: 0.0817 (0.0830)  loss_ce_unscaled: 2.3095 (2.3111)  class_error_unscaled: 61.3636 (62.3621)  loss_bbox_unscaled: 0.3937 (0.4239)  loss_giou_unscaled: 0.6049 (0.6165)  loss_conf_unscaled: 0.0082 (0.0083)
zero-shot mAP: 11.80
rare mAP: 17.23
nonrare mAP: 24.35
full mAP: 17.54

================== checkpoint0100.pth 结束：2025-09-11 09:34:43.199854 ==================

================== checkpoint0095.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0095.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:20:17  class_error: 57.78  loss: 14.7040 (14.7040)  loss_ce: 11.1800 (11.1800)  loss_bbox: 2.1841 (2.1841)  loss_giou: 1.2601 (1.2601)  loss_conf: 0.0797 (0.0797)  loss_ce_unscaled: 2.2360 (2.2360)  class_error_unscaled: 57.7778 (57.7778)  loss_bbox_unscaled: 0.4368 (0.4368)  loss_giou_unscaled: 0.6301 (0.6301)  loss_conf_unscaled: 0.0080 (0.0080)  time: 5.7145  data: 2.4517  max mem: 16066
Test:  [ 10/213]  eta: 0:04:19  class_error: 50.00  loss: 14.8780 (15.5318)  loss_ce: 11.6385 (11.9299)  loss_bbox: 2.2430 (2.2287)  loss_giou: 1.2840 (1.2851)  loss_conf: 0.0812 (0.0881)  loss_ce_unscaled: 2.3277 (2.3860)  class_error_unscaled: 64.0449 (61.4243)  loss_bbox_unscaled: 0.4486 (0.4457)  loss_giou_unscaled: 0.6420 (0.6426)  loss_conf_unscaled: 0.0081 (0.0088)  time: 1.2775  data: 0.3918  max mem: 16066
Test:  [ 20/213]  eta: 0:03:20  class_error: 74.23  loss: 15.5505 (15.5870)  loss_ce: 12.3402 (12.0720)  loss_bbox: 2.1314 (2.1748)  loss_giou: 1.2634 (1.2494)  loss_conf: 0.0825 (0.0909)  loss_ce_unscaled: 2.4680 (2.4144)  class_error_unscaled: 64.4444 (63.0373)  loss_bbox_unscaled: 0.4263 (0.4350)  loss_giou_unscaled: 0.6317 (0.6247)  loss_conf_unscaled: 0.0083 (0.0091)  time: 0.8056  data: 0.1862  max mem: 16066
Test:  [ 30/213]  eta: 0:03:01  class_error: 62.50  loss: 15.3987 (15.4865)  loss_ce: 12.0255 (12.0223)  loss_bbox: 2.1206 (2.1528)  loss_giou: 1.1858 (1.2269)  loss_conf: 0.0735 (0.0845)  loss_ce_unscaled: 2.4051 (2.4045)  class_error_unscaled: 64.7059 (63.3183)  loss_bbox_unscaled: 0.4241 (0.4306)  loss_giou_unscaled: 0.5929 (0.6135)  loss_conf_unscaled: 0.0073 (0.0085)  time: 0.8351  data: 0.2003  max mem: 16066
Test:  [ 40/213]  eta: 0:02:45  class_error: 60.64  loss: 14.9390 (15.4254)  loss_ce: 11.5795 (11.9826)  loss_bbox: 2.1186 (2.1418)  loss_giou: 1.1962 (1.2190)  loss_conf: 0.0681 (0.0820)  loss_ce_unscaled: 2.3159 (2.3965)  class_error_unscaled: 63.4409 (63.6559)  loss_bbox_unscaled: 0.4237 (0.4284)  loss_giou_unscaled: 0.5981 (0.6095)  loss_conf_unscaled: 0.0068 (0.0082)  time: 0.8647  data: 0.1971  max mem: 16066
Test:  [ 50/213]  eta: 0:02:33  class_error: 61.70  loss: 14.9390 (15.4308)  loss_ce: 11.7367 (11.9860)  loss_bbox: 2.0944 (2.1423)  loss_giou: 1.2045 (1.2207)  loss_conf: 0.0740 (0.0818)  loss_ce_unscaled: 2.3473 (2.3972)  class_error_unscaled: 61.7021 (63.0764)  loss_bbox_unscaled: 0.4189 (0.4285)  loss_giou_unscaled: 0.6023 (0.6104)  loss_conf_unscaled: 0.0074 (0.0082)  time: 0.8677  data: 0.2190  max mem: 16066
Test:  [ 60/213]  eta: 0:02:23  class_error: 61.29  loss: 15.2620 (15.3878)  loss_ce: 11.8225 (11.9644)  loss_bbox: 2.0242 (2.1235)  loss_giou: 1.2211 (1.2185)  loss_conf: 0.0742 (0.0814)  loss_ce_unscaled: 2.3645 (2.3929)  class_error_unscaled: 60.0000 (62.6318)  loss_bbox_unscaled: 0.4048 (0.4247)  loss_giou_unscaled: 0.6106 (0.6093)  loss_conf_unscaled: 0.0074 (0.0081)  time: 0.9053  data: 0.2408  max mem: 16066
Test:  [ 70/213]  eta: 0:02:14  class_error: 57.30  loss: 15.2882 (15.4187)  loss_ce: 11.9979 (12.0013)  loss_bbox: 2.0098 (2.1195)  loss_giou: 1.2167 (1.2160)  loss_conf: 0.0766 (0.0820)  loss_ce_unscaled: 2.3996 (2.4003)  class_error_unscaled: 59.7826 (62.5106)  loss_bbox_unscaled: 0.4020 (0.4239)  loss_giou_unscaled: 0.6084 (0.6080)  loss_conf_unscaled: 0.0077 (0.0082)  time: 0.9292  data: 0.2379  max mem: 16066
Test:  [ 80/213]  eta: 0:02:07  class_error: 54.26  loss: 15.5574 (15.4545)  loss_ce: 12.1905 (12.0383)  loss_bbox: 2.0499 (2.1161)  loss_giou: 1.2115 (1.2175)  loss_conf: 0.0766 (0.0826)  loss_ce_unscaled: 2.4381 (2.4077)  class_error_unscaled: 61.9565 (62.6443)  loss_bbox_unscaled: 0.4100 (0.4232)  loss_giou_unscaled: 0.6057 (0.6088)  loss_conf_unscaled: 0.0077 (0.0083)  time: 1.0135  data: 0.2999  max mem: 16066
Test:  [ 90/213]  eta: 0:01:56  class_error: 54.44  loss: 15.9899 (15.5091)  loss_ce: 12.3108 (12.0730)  loss_bbox: 2.1347 (2.1302)  loss_giou: 1.2606 (1.2230)  loss_conf: 0.0748 (0.0828)  loss_ce_unscaled: 2.4622 (2.4146)  class_error_unscaled: 63.0435 (62.6390)  loss_bbox_unscaled: 0.4269 (0.4260)  loss_giou_unscaled: 0.6303 (0.6115)  loss_conf_unscaled: 0.0075 (0.0083)  time: 0.9740  data: 0.3050  max mem: 16066
Test:  [100/213]  eta: 0:01:47  class_error: 60.22  loss: 15.9899 (15.5655)  loss_ce: 12.3108 (12.1174)  loss_bbox: 2.2392 (2.1384)  loss_giou: 1.2667 (1.2264)  loss_conf: 0.0768 (0.0834)  loss_ce_unscaled: 2.4622 (2.4235)  class_error_unscaled: 63.2653 (62.7060)  loss_bbox_unscaled: 0.4478 (0.4277)  loss_giou_unscaled: 0.6334 (0.6132)  loss_conf_unscaled: 0.0077 (0.0083)  time: 0.9169  data: 0.2588  max mem: 16066
Test:  [110/213]  eta: 0:01:38  class_error: 61.96  loss: 15.1873 (15.5244)  loss_ce: 11.5877 (12.0638)  loss_bbox: 2.2370 (2.1468)  loss_giou: 1.2825 (1.2315)  loss_conf: 0.0748 (0.0823)  loss_ce_unscaled: 2.3175 (2.4128)  class_error_unscaled: 63.2653 (62.5016)  loss_bbox_unscaled: 0.4474 (0.4294)  loss_giou_unscaled: 0.6412 (0.6158)  loss_conf_unscaled: 0.0075 (0.0082)  time: 0.9913  data: 0.2633  max mem: 16066
Test:  [120/213]  eta: 0:01:28  class_error: 69.77  loss: 15.6040 (15.5667)  loss_ce: 12.0900 (12.0967)  loss_bbox: 2.2370 (2.1526)  loss_giou: 1.2825 (1.2358)  loss_conf: 0.0746 (0.0816)  loss_ce_unscaled: 2.4180 (2.4193)  class_error_unscaled: 63.3333 (62.7132)  loss_bbox_unscaled: 0.4474 (0.4305)  loss_giou_unscaled: 0.6412 (0.6179)  loss_conf_unscaled: 0.0075 (0.0082)  time: 0.9453  data: 0.2704  max mem: 16066
Test:  [130/213]  eta: 0:01:18  class_error: 62.37  loss: 15.9483 (15.5835)  loss_ce: 12.3960 (12.1028)  loss_bbox: 2.2615 (2.1587)  loss_giou: 1.2997 (1.2409)  loss_conf: 0.0689 (0.0810)  loss_ce_unscaled: 2.4792 (2.4206)  class_error_unscaled: 64.7727 (63.0113)  loss_bbox_unscaled: 0.4523 (0.4317)  loss_giou_unscaled: 0.6499 (0.6205)  loss_conf_unscaled: 0.0069 (0.0081)  time: 0.9240  data: 0.2344  max mem: 16066
Test:  [140/213]  eta: 0:01:08  class_error: 60.82  loss: 15.1117 (15.5637)  loss_ce: 11.6747 (12.0898)  loss_bbox: 2.1700 (2.1548)  loss_giou: 1.2253 (1.2380)  loss_conf: 0.0737 (0.0812)  loss_ce_unscaled: 2.3349 (2.4180)  class_error_unscaled: 62.9213 (62.5908)  loss_bbox_unscaled: 0.4340 (0.4310)  loss_giou_unscaled: 0.6127 (0.6190)  loss_conf_unscaled: 0.0074 (0.0081)  time: 0.9217  data: 0.2298  max mem: 16066
Test:  [150/213]  eta: 0:00:59  class_error: 63.33  loss: 15.0970 (15.5494)  loss_ce: 11.3351 (12.0699)  loss_bbox: 2.1417 (2.1577)  loss_giou: 1.2253 (1.2401)  loss_conf: 0.0737 (0.0817)  loss_ce_unscaled: 2.2670 (2.4140)  class_error_unscaled: 60.2151 (62.5220)  loss_bbox_unscaled: 0.4283 (0.4315)  loss_giou_unscaled: 0.6127 (0.6200)  loss_conf_unscaled: 0.0074 (0.0082)  time: 0.9618  data: 0.2640  max mem: 16066
Test:  [160/213]  eta: 0:00:49  class_error: 56.52  loss: 15.4955 (15.5467)  loss_ce: 11.9294 (12.0718)  loss_bbox: 2.1794 (2.1528)  loss_giou: 1.2291 (1.2397)  loss_conf: 0.0770 (0.0823)  loss_ce_unscaled: 2.3859 (2.4144)  class_error_unscaled: 63.3333 (62.4929)  loss_bbox_unscaled: 0.4359 (0.4306)  loss_giou_unscaled: 0.6145 (0.6199)  loss_conf_unscaled: 0.0077 (0.0082)  time: 0.8892  data: 0.2265  max mem: 16066
Test:  [170/213]  eta: 0:00:40  class_error: 57.47  loss: 15.6163 (15.5245)  loss_ce: 12.1738 (12.0575)  loss_bbox: 2.0736 (2.1477)  loss_giou: 1.2059 (1.2370)  loss_conf: 0.0773 (0.0823)  loss_ce_unscaled: 2.4348 (2.4115)  class_error_unscaled: 62.2449 (62.4011)  loss_bbox_unscaled: 0.4147 (0.4295)  loss_giou_unscaled: 0.6030 (0.6185)  loss_conf_unscaled: 0.0077 (0.0082)  time: 0.8714  data: 0.1969  max mem: 16066
Test:  [180/213]  eta: 0:00:30  class_error: 64.00  loss: 15.7635 (15.5441)  loss_ce: 12.1486 (12.0774)  loss_bbox: 2.0518 (2.1453)  loss_giou: 1.2147 (1.2388)  loss_conf: 0.0773 (0.0826)  loss_ce_unscaled: 2.4297 (2.4155)  class_error_unscaled: 62.9630 (62.5003)  loss_bbox_unscaled: 0.4104 (0.4291)  loss_giou_unscaled: 0.6073 (0.6194)  loss_conf_unscaled: 0.0077 (0.0083)  time: 0.9193  data: 0.2301  max mem: 16066
Test:  [190/213]  eta: 0:00:21  class_error: 69.57  loss: 15.8006 (15.5516)  loss_ce: 12.2562 (12.0842)  loss_bbox: 2.0495 (2.1456)  loss_giou: 1.2391 (1.2389)  loss_conf: 0.0846 (0.0828)  loss_ce_unscaled: 2.4512 (2.4168)  class_error_unscaled: 63.3663 (62.5161)  loss_bbox_unscaled: 0.4099 (0.4291)  loss_giou_unscaled: 0.6195 (0.6194)  loss_conf_unscaled: 0.0085 (0.0083)  time: 0.9984  data: 0.2650  max mem: 16066
Test:  [200/213]  eta: 0:00:12  class_error: 69.23  loss: 15.5124 (15.5506)  loss_ce: 12.2562 (12.0843)  loss_bbox: 2.0495 (2.1444)  loss_giou: 1.2423 (1.2388)  loss_conf: 0.0827 (0.0832)  loss_ce_unscaled: 2.4512 (2.4169)  class_error_unscaled: 63.2075 (62.6352)  loss_bbox_unscaled: 0.4099 (0.4289)  loss_giou_unscaled: 0.6211 (0.6194)  loss_conf_unscaled: 0.0083 (0.0083)  time: 0.9826  data: 0.2459  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 61.36  loss: 15.8095 (15.5773)  loss_ce: 12.4418 (12.1184)  loss_bbox: 2.0224 (2.1379)  loss_giou: 1.2150 (1.2378)  loss_conf: 0.0793 (0.0831)  loss_ce_unscaled: 2.4884 (2.4237)  class_error_unscaled: 62.2449 (62.6169)  loss_bbox_unscaled: 0.4045 (0.4276)  loss_giou_unscaled: 0.6075 (0.6189)  loss_conf_unscaled: 0.0079 (0.0083)  time: 0.7752  data: 0.1994  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 62.96  loss: 15.6422 (15.5786)  loss_ce: 12.2004 (12.1213)  loss_bbox: 1.9753 (2.1362)  loss_giou: 1.2011 (1.2378)  loss_conf: 0.0793 (0.0832)  loss_ce_unscaled: 2.4401 (2.4243)  class_error_unscaled: 62.8866 (62.6729)  loss_bbox_unscaled: 0.3951 (0.4272)  loss_giou_unscaled: 0.6005 (0.6189)  loss_conf_unscaled: 0.0079 (0.0083)  time: 0.8897  data: 0.1893  max mem: 16066
Test: Total time: 0:03:20 (0.9423 s / it)
Averaged stats: class_error: 62.96  loss: 15.6422 (15.5786)  loss_ce: 12.2004 (12.1213)  loss_bbox: 1.9753 (2.1362)  loss_giou: 1.2011 (1.2378)  loss_conf: 0.0793 (0.0832)  loss_ce_unscaled: 2.4401 (2.4243)  class_error_unscaled: 62.8866 (62.6729)  loss_bbox_unscaled: 0.3951 (0.4272)  loss_giou_unscaled: 0.6005 (0.6189)  loss_conf_unscaled: 0.0079 (0.0083)
zero-shot mAP: 11.95
rare mAP: 17.04
nonrare mAP: 24.18
full mAP: 17.45

================== checkpoint0095.pth 结束：2025-09-11 09:38:54.746720 ==================

================== checkpoint0090.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0090.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:16:33  class_error: 61.11  loss: 15.0784 (15.0784)  loss_ce: 11.6210 (11.6210)  loss_bbox: 2.1318 (2.1318)  loss_giou: 1.2614 (1.2614)  loss_conf: 0.0642 (0.0642)  loss_ce_unscaled: 2.3242 (2.3242)  class_error_unscaled: 61.1111 (61.1111)  loss_bbox_unscaled: 0.4264 (0.4264)  loss_giou_unscaled: 0.6307 (0.6307)  loss_conf_unscaled: 0.0064 (0.0064)  time: 4.6640  data: 1.7756  max mem: 16066
Test:  [ 10/213]  eta: 0:03:50  class_error: 48.94  loss: 15.3738 (15.7360)  loss_ce: 12.0174 (12.1485)  loss_bbox: 2.1999 (2.2058)  loss_giou: 1.2971 (1.2907)  loss_conf: 0.0729 (0.0911)  loss_ce_unscaled: 2.4035 (2.4297)  class_error_unscaled: 62.5000 (61.8014)  loss_bbox_unscaled: 0.4400 (0.4412)  loss_giou_unscaled: 0.6486 (0.6453)  loss_conf_unscaled: 0.0073 (0.0091)  time: 1.1352  data: 0.3555  max mem: 16066
Test:  [ 20/213]  eta: 0:03:00  class_error: 75.26  loss: 15.8975 (15.7493)  loss_ce: 12.3364 (12.2317)  loss_bbox: 2.0937 (2.1716)  loss_giou: 1.2437 (1.2566)  loss_conf: 0.0763 (0.0894)  loss_ce_unscaled: 2.4673 (2.4463)  class_error_unscaled: 63.2184 (63.3460)  loss_bbox_unscaled: 0.4187 (0.4343)  loss_giou_unscaled: 0.6219 (0.6283)  loss_conf_unscaled: 0.0076 (0.0089)  time: 0.7512  data: 0.1838  max mem: 16066
Test:  [ 30/213]  eta: 0:02:45  class_error: 62.50  loss: 16.2481 (15.7353)  loss_ce: 12.7762 (12.2749)  loss_bbox: 2.0719 (2.1470)  loss_giou: 1.1979 (1.2301)  loss_conf: 0.0722 (0.0834)  loss_ce_unscaled: 2.5552 (2.4550)  class_error_unscaled: 64.2857 (63.6372)  loss_bbox_unscaled: 0.4144 (0.4294)  loss_giou_unscaled: 0.5990 (0.6151)  loss_conf_unscaled: 0.0072 (0.0083)  time: 0.7751  data: 0.1724  max mem: 16066
Test:  [ 40/213]  eta: 0:02:29  class_error: 61.70  loss: 15.7053 (15.6945)  loss_ce: 12.2456 (12.2471)  loss_bbox: 2.0675 (2.1410)  loss_giou: 1.1947 (1.2249)  loss_conf: 0.0666 (0.0816)  loss_ce_unscaled: 2.4491 (2.4494)  class_error_unscaled: 64.2857 (64.3347)  loss_bbox_unscaled: 0.4135 (0.4282)  loss_giou_unscaled: 0.5974 (0.6124)  loss_conf_unscaled: 0.0067 (0.0082)  time: 0.7914  data: 0.1747  max mem: 16066
Test:  [ 50/213]  eta: 0:02:17  class_error: 60.64  loss: 15.6616 (15.7169)  loss_ce: 12.2456 (12.2764)  loss_bbox: 2.0636 (2.1346)  loss_giou: 1.1986 (1.2248)  loss_conf: 0.0719 (0.0811)  loss_ce_unscaled: 2.4491 (2.4553)  class_error_unscaled: 62.2222 (63.4342)  loss_bbox_unscaled: 0.4127 (0.4269)  loss_giou_unscaled: 0.5993 (0.6124)  loss_conf_unscaled: 0.0072 (0.0081)  time: 0.7521  data: 0.1902  max mem: 16066
Test:  [ 60/213]  eta: 0:02:06  class_error: 62.37  loss: 15.4004 (15.6685)  loss_ce: 11.9963 (12.2422)  loss_bbox: 2.0573 (2.1225)  loss_giou: 1.2462 (1.2232)  loss_conf: 0.0831 (0.0806)  loss_ce_unscaled: 2.3993 (2.4484)  class_error_unscaled: 60.0000 (63.0361)  loss_bbox_unscaled: 0.4115 (0.4245)  loss_giou_unscaled: 0.6231 (0.6116)  loss_conf_unscaled: 0.0083 (0.0081)  time: 0.7464  data: 0.1873  max mem: 16066
Test:  [ 70/213]  eta: 0:01:56  class_error: 59.55  loss: 15.4558 (15.7030)  loss_ce: 12.2029 (12.2796)  loss_bbox: 2.0117 (2.1211)  loss_giou: 1.2219 (1.2214)  loss_conf: 0.0815 (0.0809)  loss_ce_unscaled: 2.4406 (2.4559)  class_error_unscaled: 60.6742 (62.8549)  loss_bbox_unscaled: 0.4023 (0.4242)  loss_giou_unscaled: 0.6109 (0.6107)  loss_conf_unscaled: 0.0082 (0.0081)  time: 0.7497  data: 0.1536  max mem: 16066
Test:  [ 80/213]  eta: 0:01:48  class_error: 55.32  loss: 15.8615 (15.7423)  loss_ce: 12.3098 (12.3193)  loss_bbox: 2.0404 (2.1173)  loss_giou: 1.2168 (1.2242)  loss_conf: 0.0813 (0.0815)  loss_ce_unscaled: 2.4620 (2.4639)  class_error_unscaled: 61.9565 (63.0814)  loss_bbox_unscaled: 0.4081 (0.4235)  loss_giou_unscaled: 0.6084 (0.6121)  loss_conf_unscaled: 0.0081 (0.0082)  time: 0.7693  data: 0.1560  max mem: 16066
Test:  [ 90/213]  eta: 0:01:39  class_error: 54.44  loss: 15.8761 (15.7923)  loss_ce: 12.3098 (12.3508)  loss_bbox: 2.1606 (2.1317)  loss_giou: 1.2587 (1.2286)  loss_conf: 0.0734 (0.0812)  loss_ce_unscaled: 2.4620 (2.4702)  class_error_unscaled: 62.9213 (63.0605)  loss_bbox_unscaled: 0.4321 (0.4263)  loss_giou_unscaled: 0.6293 (0.6143)  loss_conf_unscaled: 0.0073 (0.0081)  time: 0.7808  data: 0.1594  max mem: 16066
Test:  [100/213]  eta: 0:01:29  class_error: 61.29  loss: 15.9033 (15.8595)  loss_ce: 12.4053 (12.4037)  loss_bbox: 2.2955 (2.1426)  loss_giou: 1.2628 (1.2317)  loss_conf: 0.0810 (0.0816)  loss_ce_unscaled: 2.4811 (2.4807)  class_error_unscaled: 62.3656 (63.0647)  loss_bbox_unscaled: 0.4591 (0.4285)  loss_giou_unscaled: 0.6314 (0.6158)  loss_conf_unscaled: 0.0081 (0.0082)  time: 0.7280  data: 0.1569  max mem: 16066
Test:  [110/213]  eta: 0:01:21  class_error: 67.39  loss: 15.7800 (15.8142)  loss_ce: 12.1733 (12.3471)  loss_bbox: 2.2585 (2.1492)  loss_giou: 1.2797 (1.2370)  loss_conf: 0.0813 (0.0809)  loss_ce_unscaled: 2.4347 (2.4694)  class_error_unscaled: 63.5294 (63.0146)  loss_bbox_unscaled: 0.4517 (0.4298)  loss_giou_unscaled: 0.6399 (0.6185)  loss_conf_unscaled: 0.0081 (0.0081)  time: 0.7254  data: 0.1489  max mem: 16066
Test:  [120/213]  eta: 0:01:13  class_error: 68.60  loss: 16.0039 (15.8405)  loss_ce: 12.4065 (12.3627)  loss_bbox: 2.1541 (2.1565)  loss_giou: 1.2820 (1.2416)  loss_conf: 0.0659 (0.0797)  loss_ce_unscaled: 2.4813 (2.4725)  class_error_unscaled: 63.7255 (63.1936)  loss_bbox_unscaled: 0.4308 (0.4313)  loss_giou_unscaled: 0.6410 (0.6208)  loss_conf_unscaled: 0.0066 (0.0080)  time: 0.7611  data: 0.1432  max mem: 16066
Test:  [130/213]  eta: 0:01:04  class_error: 60.22  loss: 16.1322 (15.8487)  loss_ce: 12.3146 (12.3581)  loss_bbox: 2.2162 (2.1645)  loss_giou: 1.3283 (1.2470)  loss_conf: 0.0694 (0.0790)  loss_ce_unscaled: 2.4629 (2.4716)  class_error_unscaled: 66.6667 (63.5387)  loss_bbox_unscaled: 0.4432 (0.4329)  loss_giou_unscaled: 0.6642 (0.6235)  loss_conf_unscaled: 0.0069 (0.0079)  time: 0.7130  data: 0.1499  max mem: 16066
Test:  [140/213]  eta: 0:00:57  class_error: 58.76  loss: 15.5820 (15.8483)  loss_ce: 11.9874 (12.3633)  loss_bbox: 2.1770 (2.1615)  loss_giou: 1.2166 (1.2444)  loss_conf: 0.0707 (0.0792)  loss_ce_unscaled: 2.3975 (2.4727)  class_error_unscaled: 64.4444 (63.2695)  loss_bbox_unscaled: 0.4354 (0.4323)  loss_giou_unscaled: 0.6083 (0.6222)  loss_conf_unscaled: 0.0071 (0.0079)  time: 0.7379  data: 0.1552  max mem: 16066
Test:  [150/213]  eta: 0:00:48  class_error: 64.44  loss: 15.5191 (15.8279)  loss_ce: 11.9874 (12.3419)  loss_bbox: 2.1547 (2.1604)  loss_giou: 1.2186 (1.2453)  loss_conf: 0.0850 (0.0803)  loss_ce_unscaled: 2.3975 (2.4684)  class_error_unscaled: 59.7701 (63.1775)  loss_bbox_unscaled: 0.4309 (0.4321)  loss_giou_unscaled: 0.6093 (0.6227)  loss_conf_unscaled: 0.0085 (0.0080)  time: 0.7404  data: 0.1532  max mem: 16066
Test:  [160/213]  eta: 0:00:41  class_error: 54.35  loss: 15.5945 (15.8247)  loss_ce: 12.0366 (12.3424)  loss_bbox: 2.1547 (2.1567)  loss_giou: 1.2230 (1.2450)  loss_conf: 0.0855 (0.0806)  loss_ce_unscaled: 2.4073 (2.4685)  class_error_unscaled: 62.3656 (63.0128)  loss_bbox_unscaled: 0.4309 (0.4313)  loss_giou_unscaled: 0.6115 (0.6225)  loss_conf_unscaled: 0.0086 (0.0081)  time: 0.7447  data: 0.1568  max mem: 16066
Test:  [170/213]  eta: 0:00:33  class_error: 58.62  loss: 15.5909 (15.7910)  loss_ce: 12.0837 (12.3170)  loss_bbox: 2.0859 (2.1508)  loss_giou: 1.2117 (1.2423)  loss_conf: 0.0835 (0.0809)  loss_ce_unscaled: 2.4167 (2.4634)  class_error_unscaled: 59.7826 (62.8128)  loss_bbox_unscaled: 0.4172 (0.4302)  loss_giou_unscaled: 0.6059 (0.6212)  loss_conf_unscaled: 0.0084 (0.0081)  time: 0.7432  data: 0.1536  max mem: 16066
Test:  [180/213]  eta: 0:00:25  class_error: 62.00  loss: 15.4381 (15.7894)  loss_ce: 12.0837 (12.3152)  loss_bbox: 2.0751 (2.1491)  loss_giou: 1.2302 (1.2437)  loss_conf: 0.0833 (0.0814)  loss_ce_unscaled: 2.4167 (2.4630)  class_error_unscaled: 61.2245 (62.8758)  loss_bbox_unscaled: 0.4150 (0.4298)  loss_giou_unscaled: 0.6151 (0.6218)  loss_conf_unscaled: 0.0083 (0.0081)  time: 0.7585  data: 0.1522  max mem: 16066
Test:  [190/213]  eta: 0:00:17  class_error: 68.48  loss: 15.6745 (15.7906)  loss_ce: 12.4657 (12.3175)  loss_bbox: 1.9963 (2.1480)  loss_giou: 1.2302 (1.2434)  loss_conf: 0.0833 (0.0817)  loss_ce_unscaled: 2.4931 (2.4635)  class_error_unscaled: 62.8866 (62.8236)  loss_bbox_unscaled: 0.3993 (0.4296)  loss_giou_unscaled: 0.6151 (0.6217)  loss_conf_unscaled: 0.0083 (0.0082)  time: 0.7568  data: 0.1545  max mem: 16066
Test:  [200/213]  eta: 0:00:10  class_error: 69.23  loss: 15.6745 (15.7981)  loss_ce: 12.4657 (12.3239)  loss_bbox: 2.0577 (2.1485)  loss_giou: 1.2500 (1.2439)  loss_conf: 0.0816 (0.0818)  loss_ce_unscaled: 2.4931 (2.4648)  class_error_unscaled: 63.2653 (62.9036)  loss_bbox_unscaled: 0.4115 (0.4297)  loss_giou_unscaled: 0.6250 (0.6220)  loss_conf_unscaled: 0.0082 (0.0082)  time: 0.7721  data: 0.1526  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 60.23  loss: 16.4818 (15.8335)  loss_ce: 12.8664 (12.3660)  loss_bbox: 2.0577 (2.1428)  loss_giou: 1.2327 (1.2430)  loss_conf: 0.0704 (0.0816)  loss_ce_unscaled: 2.5733 (2.4732)  class_error_unscaled: 63.2653 (62.8870)  loss_bbox_unscaled: 0.4115 (0.4286)  loss_giou_unscaled: 0.6164 (0.6215)  loss_conf_unscaled: 0.0070 (0.0082)  time: 0.7943  data: 0.1619  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 62.96  loss: 16.2010 (15.8300)  loss_ce: 12.4038 (12.3641)  loss_bbox: 1.9969 (2.1411)  loss_giou: 1.2237 (1.2432)  loss_conf: 0.0791 (0.0816)  loss_ce_unscaled: 2.4808 (2.4728)  class_error_unscaled: 63.2653 (62.9405)  loss_bbox_unscaled: 0.3994 (0.4282)  loss_giou_unscaled: 0.6118 (0.6216)  loss_conf_unscaled: 0.0079 (0.0082)  time: 0.7792  data: 0.1609  max mem: 16066
Test: Total time: 0:02:44 (0.7717 s / it)
Averaged stats: class_error: 62.96  loss: 16.2010 (15.8300)  loss_ce: 12.4038 (12.3641)  loss_bbox: 1.9969 (2.1411)  loss_giou: 1.2237 (1.2432)  loss_conf: 0.0791 (0.0816)  loss_ce_unscaled: 2.4808 (2.4728)  class_error_unscaled: 63.2653 (62.9405)  loss_bbox_unscaled: 0.3994 (0.4282)  loss_giou_unscaled: 0.6118 (0.6216)  loss_conf_unscaled: 0.0079 (0.0082)
zero-shot mAP: 11.82
rare mAP: 17.09
nonrare mAP: 24.29
full mAP: 17.47

================== checkpoint0090.pth 结束：2025-09-11 09:42:26.292039 ==================

================== checkpoint0085.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0085.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:22:59  class_error: 53.33  loss: 14.6117 (14.6117)  loss_ce: 11.0806 (11.0806)  loss_bbox: 2.1627 (2.1627)  loss_giou: 1.2779 (1.2779)  loss_conf: 0.0905 (0.0905)  loss_ce_unscaled: 2.2161 (2.2161)  class_error_unscaled: 53.3333 (53.3333)  loss_bbox_unscaled: 0.4325 (0.4325)  loss_giou_unscaled: 0.6390 (0.6390)  loss_conf_unscaled: 0.0090 (0.0090)  time: 6.4759  data: 2.5047  max mem: 16066
Test:  [ 10/213]  eta: 0:04:34  class_error: 48.94  loss: 15.4094 (15.6532)  loss_ce: 12.2299 (12.0709)  loss_bbox: 2.1818 (2.2107)  loss_giou: 1.2779 (1.2838)  loss_conf: 0.0807 (0.0878)  loss_ce_unscaled: 2.4460 (2.4142)  class_error_unscaled: 62.1359 (61.6605)  loss_bbox_unscaled: 0.4364 (0.4421)  loss_giou_unscaled: 0.6390 (0.6419)  loss_conf_unscaled: 0.0081 (0.0088)  time: 1.3504  data: 0.4202  max mem: 16066
Test:  [ 20/213]  eta: 0:03:31  class_error: 74.23  loss: 15.5189 (15.7549)  loss_ce: 12.2301 (12.2456)  loss_bbox: 2.1516 (2.1720)  loss_giou: 1.2478 (1.2489)  loss_conf: 0.0799 (0.0884)  loss_ce_unscaled: 2.4460 (2.4491)  class_error_unscaled: 62.5000 (62.6672)  loss_bbox_unscaled: 0.4303 (0.4344)  loss_giou_unscaled: 0.6239 (0.6245)  loss_conf_unscaled: 0.0080 (0.0088)  time: 0.8246  data: 0.1923  max mem: 16066
Test:  [ 30/213]  eta: 0:03:05  class_error: 62.50  loss: 15.4842 (15.6898)  loss_ce: 11.9670 (12.2267)  loss_bbox: 2.1275 (2.1530)  loss_giou: 1.1687 (1.2270)  loss_conf: 0.0721 (0.0832)  loss_ce_unscaled: 2.3934 (2.4453)  class_error_unscaled: 64.2857 (62.9586)  loss_bbox_unscaled: 0.4255 (0.4306)  loss_giou_unscaled: 0.5844 (0.6135)  loss_conf_unscaled: 0.0072 (0.0083)  time: 0.8313  data: 0.2369  max mem: 16066
Test:  [ 40/213]  eta: 0:02:48  class_error: 60.64  loss: 15.1195 (15.6200)  loss_ce: 11.9025 (12.1731)  loss_bbox: 2.1275 (2.1448)  loss_giou: 1.1687 (1.2191)  loss_conf: 0.0721 (0.0831)  loss_ce_unscaled: 2.3805 (2.4346)  class_error_unscaled: 63.5294 (63.4848)  loss_bbox_unscaled: 0.4255 (0.4290)  loss_giou_unscaled: 0.5844 (0.6095)  loss_conf_unscaled: 0.0072 (0.0083)  time: 0.8480  data: 0.2482  max mem: 16066
Test:  [ 50/213]  eta: 0:02:38  class_error: 62.77  loss: 15.4232 (15.6028)  loss_ce: 11.9341 (12.1570)  loss_bbox: 2.1610 (2.1439)  loss_giou: 1.2311 (1.2210)  loss_conf: 0.0794 (0.0809)  loss_ce_unscaled: 2.3868 (2.4314)  class_error_unscaled: 61.9565 (62.9678)  loss_bbox_unscaled: 0.4322 (0.4288)  loss_giou_unscaled: 0.6155 (0.6105)  loss_conf_unscaled: 0.0079 (0.0081)  time: 0.9083  data: 0.2342  max mem: 16066
Test:  [ 60/213]  eta: 0:02:28  class_error: 59.14  loss: 15.4510 (15.5732)  loss_ce: 11.9170 (12.1401)  loss_bbox: 2.0301 (2.1323)  loss_giou: 1.2311 (1.2206)  loss_conf: 0.0726 (0.0802)  loss_ce_unscaled: 2.3834 (2.4280)  class_error_unscaled: 59.7826 (62.5708)  loss_bbox_unscaled: 0.4060 (0.4265)  loss_giou_unscaled: 0.6155 (0.6103)  loss_conf_unscaled: 0.0073 (0.0080)  time: 0.9689  data: 0.2615  max mem: 16066
Test:  [ 70/213]  eta: 0:02:17  class_error: 61.80  loss: 15.3144 (15.6026)  loss_ce: 11.9170 (12.1723)  loss_bbox: 2.0301 (2.1313)  loss_giou: 1.2277 (1.2184)  loss_conf: 0.0791 (0.0806)  loss_ce_unscaled: 2.3834 (2.4345)  class_error_unscaled: 59.7826 (62.6187)  loss_bbox_unscaled: 0.4060 (0.4263)  loss_giou_unscaled: 0.6139 (0.6092)  loss_conf_unscaled: 0.0079 (0.0081)  time: 0.9323  data: 0.2387  max mem: 16066
Test:  [ 80/213]  eta: 0:02:05  class_error: 55.32  loss: 15.3144 (15.6397)  loss_ce: 11.9680 (12.2096)  loss_bbox: 2.1065 (2.1281)  loss_giou: 1.2214 (1.2215)  loss_conf: 0.0780 (0.0805)  loss_ce_unscaled: 2.3936 (2.4419)  class_error_unscaled: 62.9213 (62.8972)  loss_bbox_unscaled: 0.4213 (0.4256)  loss_giou_unscaled: 0.6107 (0.6108)  loss_conf_unscaled: 0.0078 (0.0081)  time: 0.8510  data: 0.2325  max mem: 16066
Test:  [ 90/213]  eta: 0:01:55  class_error: 54.44  loss: 15.9291 (15.6724)  loss_ce: 12.3556 (12.2251)  loss_bbox: 2.1373 (2.1406)  loss_giou: 1.2661 (1.2262)  loss_conf: 0.0767 (0.0804)  loss_ce_unscaled: 2.4711 (2.4450)  class_error_unscaled: 63.0435 (62.9684)  loss_bbox_unscaled: 0.4275 (0.4281)  loss_giou_unscaled: 0.6331 (0.6131)  loss_conf_unscaled: 0.0077 (0.0080)  time: 0.8596  data: 0.2210  max mem: 16066
Test:  [100/213]  eta: 0:01:46  class_error: 67.74  loss: 15.9563 (15.7340)  loss_ce: 12.4573 (12.2699)  loss_bbox: 2.3123 (2.1533)  loss_giou: 1.2715 (1.2302)  loss_conf: 0.0795 (0.0806)  loss_ce_unscaled: 2.4915 (2.4540)  class_error_unscaled: 64.8936 (63.1018)  loss_bbox_unscaled: 0.4625 (0.4307)  loss_giou_unscaled: 0.6358 (0.6151)  loss_conf_unscaled: 0.0079 (0.0081)  time: 0.9398  data: 0.2158  max mem: 16066
Test:  [110/213]  eta: 0:01:36  class_error: 67.39  loss: 15.7861 (15.7157)  loss_ce: 12.1956 (12.2385)  loss_bbox: 2.3123 (2.1624)  loss_giou: 1.2715 (1.2354)  loss_conf: 0.0717 (0.0794)  loss_ce_unscaled: 2.4391 (2.4477)  class_error_unscaled: 64.6465 (63.0159)  loss_bbox_unscaled: 0.4625 (0.4325)  loss_giou_unscaled: 0.6358 (0.6177)  loss_conf_unscaled: 0.0072 (0.0079)  time: 0.9112  data: 0.2001  max mem: 16066
Test:  [120/213]  eta: 0:01:25  class_error: 65.12  loss: 16.0740 (15.7684)  loss_ce: 12.4672 (12.2858)  loss_bbox: 2.1966 (2.1651)  loss_giou: 1.2989 (1.2391)  loss_conf: 0.0676 (0.0785)  loss_ce_unscaled: 2.4934 (2.4572)  class_error_unscaled: 64.6465 (63.2139)  loss_bbox_unscaled: 0.4393 (0.4330)  loss_giou_unscaled: 0.6495 (0.6195)  loss_conf_unscaled: 0.0068 (0.0078)  time: 0.7919  data: 0.1846  max mem: 16066
Test:  [130/213]  eta: 0:01:16  class_error: 64.52  loss: 16.0740 (15.7709)  loss_ce: 12.4672 (12.2760)  loss_bbox: 2.2929 (2.1730)  loss_giou: 1.3020 (1.2443)  loss_conf: 0.0649 (0.0776)  loss_ce_unscaled: 2.4934 (2.4552)  class_error_unscaled: 65.1685 (63.5664)  loss_bbox_unscaled: 0.4586 (0.4346)  loss_giou_unscaled: 0.6510 (0.6222)  loss_conf_unscaled: 0.0065 (0.0078)  time: 0.8150  data: 0.1956  max mem: 16066
Test:  [140/213]  eta: 0:01:05  class_error: 65.98  loss: 15.5666 (15.7728)  loss_ce: 11.8226 (12.2852)  loss_bbox: 2.2115 (2.1676)  loss_giou: 1.2063 (1.2420)  loss_conf: 0.0721 (0.0779)  loss_ce_unscaled: 2.3645 (2.4570)  class_error_unscaled: 63.8298 (63.2597)  loss_bbox_unscaled: 0.4423 (0.4335)  loss_giou_unscaled: 0.6032 (0.6210)  loss_conf_unscaled: 0.0072 (0.0078)  time: 0.8141  data: 0.1808  max mem: 16066
Test:  [150/213]  eta: 0:00:56  class_error: 65.56  loss: 15.6095 (15.7610)  loss_ce: 11.8719 (12.2678)  loss_bbox: 2.1942 (2.1708)  loss_giou: 1.2153 (1.2439)  loss_conf: 0.0776 (0.0786)  loss_ce_unscaled: 2.3744 (2.4536)  class_error_unscaled: 60.2151 (63.1893)  loss_bbox_unscaled: 0.4388 (0.4342)  loss_giou_unscaled: 0.6077 (0.6219)  loss_conf_unscaled: 0.0078 (0.0079)  time: 0.8014  data: 0.1659  max mem: 16066
Test:  [160/213]  eta: 0:00:47  class_error: 54.35  loss: 15.3279 (15.7389)  loss_ce: 11.8719 (12.2507)  loss_bbox: 2.1942 (2.1655)  loss_giou: 1.2331 (1.2437)  loss_conf: 0.0812 (0.0790)  loss_ce_unscaled: 2.3744 (2.4501)  class_error_unscaled: 62.7660 (63.0834)  loss_bbox_unscaled: 0.4388 (0.4331)  loss_giou_unscaled: 0.6165 (0.6219)  loss_conf_unscaled: 0.0081 (0.0079)  time: 0.8008  data: 0.1671  max mem: 16066
Test:  [170/213]  eta: 0:00:38  class_error: 59.77  loss: 15.2524 (15.7153)  loss_ce: 11.9065 (12.2357)  loss_bbox: 2.0659 (2.1600)  loss_giou: 1.1987 (1.2408)  loss_conf: 0.0767 (0.0788)  loss_ce_unscaled: 2.3813 (2.4471)  class_error_unscaled: 62.2449 (62.9295)  loss_bbox_unscaled: 0.4132 (0.4320)  loss_giou_unscaled: 0.5994 (0.6204)  loss_conf_unscaled: 0.0077 (0.0079)  time: 0.8372  data: 0.1774  max mem: 16066
Test:  [180/213]  eta: 0:00:29  class_error: 65.00  loss: 15.5356 (15.7227)  loss_ce: 12.3632 (12.2448)  loss_bbox: 2.0079 (2.1560)  loss_giou: 1.2317 (1.2423)  loss_conf: 0.0763 (0.0795)  loss_ce_unscaled: 2.4726 (2.4490)  class_error_unscaled: 62.3762 (63.0651)  loss_bbox_unscaled: 0.4016 (0.4312)  loss_giou_unscaled: 0.6159 (0.6212)  loss_conf_unscaled: 0.0076 (0.0079)  time: 0.8502  data: 0.1843  max mem: 16066
Test:  [190/213]  eta: 0:00:20  class_error: 66.30  loss: 15.5693 (15.7330)  loss_ce: 12.4520 (12.2553)  loss_bbox: 2.1074 (2.1564)  loss_giou: 1.2481 (1.2421)  loss_conf: 0.0844 (0.0793)  loss_ce_unscaled: 2.4904 (2.4511)  class_error_unscaled: 63.5294 (63.0065)  loss_bbox_unscaled: 0.4215 (0.4313)  loss_giou_unscaled: 0.6241 (0.6210)  loss_conf_unscaled: 0.0084 (0.0079)  time: 0.8555  data: 0.1791  max mem: 16066
Test:  [200/213]  eta: 0:00:11  class_error: 68.13  loss: 15.3493 (15.7230)  loss_ce: 12.1043 (12.2463)  loss_bbox: 2.1074 (2.1556)  loss_giou: 1.2329 (1.2417)  loss_conf: 0.0757 (0.0794)  loss_ce_unscaled: 2.4209 (2.4493)  class_error_unscaled: 64.5570 (63.0684)  loss_bbox_unscaled: 0.4215 (0.4311)  loss_giou_unscaled: 0.6165 (0.6208)  loss_conf_unscaled: 0.0076 (0.0079)  time: 0.8698  data: 0.1907  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 63.64  loss: 15.7099 (15.7474)  loss_ce: 12.2323 (12.2770)  loss_bbox: 2.0834 (2.1500)  loss_giou: 1.2180 (1.2413)  loss_conf: 0.0707 (0.0791)  loss_ce_unscaled: 2.4465 (2.4554)  class_error_unscaled: 63.9175 (63.0953)  loss_bbox_unscaled: 0.4167 (0.4300)  loss_giou_unscaled: 0.6090 (0.6206)  loss_conf_unscaled: 0.0071 (0.0079)  time: 0.9922  data: 0.1986  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 62.96  loss: 15.5270 (15.7517)  loss_ce: 12.1043 (12.2803)  loss_bbox: 2.0814 (2.1503)  loss_giou: 1.2349 (1.2421)  loss_conf: 0.0707 (0.0791)  loss_ce_unscaled: 2.4209 (2.4561)  class_error_unscaled: 63.6364 (63.1367)  loss_bbox_unscaled: 0.4163 (0.4301)  loss_giou_unscaled: 0.6175 (0.6210)  loss_conf_unscaled: 0.0071 (0.0079)  time: 0.9640  data: 0.1903  max mem: 16066
Test: Total time: 0:03:11 (0.8968 s / it)
Averaged stats: class_error: 62.96  loss: 15.5270 (15.7517)  loss_ce: 12.1043 (12.2803)  loss_bbox: 2.0814 (2.1503)  loss_giou: 1.2349 (1.2421)  loss_conf: 0.0707 (0.0791)  loss_ce_unscaled: 2.4209 (2.4561)  class_error_unscaled: 63.6364 (63.1367)  loss_bbox_unscaled: 0.4163 (0.4301)  loss_giou_unscaled: 0.6175 (0.6210)  loss_conf_unscaled: 0.0071 (0.0079)
zero-shot mAP: 11.85
rare mAP: 16.85
nonrare mAP: 24.20
full mAP: 17.33

================== checkpoint0085.pth 结束：2025-09-11 09:46:27.881453 ==================

================== checkpoint0080.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0080.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:21:04  class_error: 56.67  loss: 14.9526 (14.9526)  loss_ce: 11.5412 (11.5412)  loss_bbox: 2.0652 (2.0652)  loss_giou: 1.2575 (1.2575)  loss_conf: 0.0888 (0.0888)  loss_ce_unscaled: 2.3082 (2.3082)  class_error_unscaled: 56.6667 (56.6667)  loss_bbox_unscaled: 0.4130 (0.4130)  loss_giou_unscaled: 0.6287 (0.6287)  loss_conf_unscaled: 0.0089 (0.0089)  time: 5.9343  data: 2.7706  max mem: 16066
Test:  [ 10/213]  eta: 0:04:12  class_error: 54.26  loss: 16.1427 (16.4321)  loss_ce: 12.7609 (12.8184)  loss_bbox: 2.2929 (2.2278)  loss_giou: 1.2922 (1.3022)  loss_conf: 0.0801 (0.0837)  loss_ce_unscaled: 2.5522 (2.5637)  class_error_unscaled: 63.3333 (61.5016)  loss_bbox_unscaled: 0.4586 (0.4456)  loss_giou_unscaled: 0.6461 (0.6511)  loss_conf_unscaled: 0.0080 (0.0084)  time: 1.2414  data: 0.4136  max mem: 16066
Test:  [ 20/213]  eta: 0:03:16  class_error: 74.23  loss: 16.5822 (16.4486)  loss_ce: 12.9610 (12.9338)  loss_bbox: 2.1396 (2.1630)  loss_giou: 1.2571 (1.2632)  loss_conf: 0.0802 (0.0885)  loss_ce_unscaled: 2.5922 (2.5868)  class_error_unscaled: 64.0449 (63.3349)  loss_bbox_unscaled: 0.4279 (0.4326)  loss_giou_unscaled: 0.6286 (0.6316)  loss_conf_unscaled: 0.0080 (0.0089)  time: 0.7750  data: 0.1660  max mem: 16066
Test:  [ 30/213]  eta: 0:02:51  class_error: 60.23  loss: 16.6643 (16.3993)  loss_ce: 13.0875 (12.9401)  loss_bbox: 2.0538 (2.1413)  loss_giou: 1.2139 (1.2341)  loss_conf: 0.0821 (0.0838)  loss_ce_unscaled: 2.6175 (2.5880)  class_error_unscaled: 64.7059 (63.4260)  loss_bbox_unscaled: 0.4108 (0.4283)  loss_giou_unscaled: 0.6070 (0.6170)  loss_conf_unscaled: 0.0082 (0.0084)  time: 0.7737  data: 0.1569  max mem: 16066
Test:  [ 40/213]  eta: 0:02:37  class_error: 68.09  loss: 16.2099 (16.3664)  loss_ce: 13.0340 (12.9206)  loss_bbox: 2.1811 (2.1374)  loss_giou: 1.1681 (1.2274)  loss_conf: 0.0714 (0.0810)  loss_ce_unscaled: 2.6068 (2.5841)  class_error_unscaled: 65.5914 (64.0305)  loss_bbox_unscaled: 0.4362 (0.4275)  loss_giou_unscaled: 0.5840 (0.6137)  loss_conf_unscaled: 0.0071 (0.0081)  time: 0.7914  data: 0.1875  max mem: 16066
Test:  [ 50/213]  eta: 0:02:24  class_error: 60.64  loss: 16.1154 (16.3020)  loss_ce: 12.9522 (12.8565)  loss_bbox: 2.0877 (2.1369)  loss_giou: 1.2595 (1.2291)  loss_conf: 0.0719 (0.0795)  loss_ce_unscaled: 2.5904 (2.5713)  class_error_unscaled: 62.2222 (63.2850)  loss_bbox_unscaled: 0.4175 (0.4274)  loss_giou_unscaled: 0.6298 (0.6146)  loss_conf_unscaled: 0.0072 (0.0079)  time: 0.8119  data: 0.2017  max mem: 16066
Test:  [ 60/213]  eta: 0:02:13  class_error: 62.37  loss: 15.5429 (16.2505)  loss_ce: 12.1819 (12.8218)  loss_bbox: 2.0731 (2.1209)  loss_giou: 1.2533 (1.2281)  loss_conf: 0.0719 (0.0797)  loss_ce_unscaled: 2.4364 (2.5644)  class_error_unscaled: 60.6383 (62.9330)  loss_bbox_unscaled: 0.4146 (0.4242)  loss_giou_unscaled: 0.6266 (0.6141)  loss_conf_unscaled: 0.0072 (0.0080)  time: 0.7917  data: 0.1731  max mem: 16066
Test:  [ 70/213]  eta: 0:02:02  class_error: 64.04  loss: 16.1540 (16.3052)  loss_ce: 12.8065 (12.8778)  loss_bbox: 2.0428 (2.1200)  loss_giou: 1.2335 (1.2264)  loss_conf: 0.0852 (0.0810)  loss_ce_unscaled: 2.5613 (2.5756)  class_error_unscaled: 62.3656 (63.0212)  loss_bbox_unscaled: 0.4086 (0.4240)  loss_giou_unscaled: 0.6167 (0.6132)  loss_conf_unscaled: 0.0085 (0.0081)  time: 0.7779  data: 0.1556  max mem: 16066
Test:  [ 80/213]  eta: 0:01:53  class_error: 53.19  loss: 16.2443 (16.3767)  loss_ce: 12.9308 (12.9470)  loss_bbox: 2.0927 (2.1180)  loss_giou: 1.2346 (1.2298)  loss_conf: 0.0852 (0.0818)  loss_ce_unscaled: 2.5862 (2.5894)  class_error_unscaled: 62.9213 (62.9523)  loss_bbox_unscaled: 0.4185 (0.4236)  loss_giou_unscaled: 0.6173 (0.6149)  loss_conf_unscaled: 0.0085 (0.0082)  time: 0.8079  data: 0.1560  max mem: 16066
Test:  [ 90/213]  eta: 0:01:44  class_error: 55.56  loss: 16.8571 (16.4327)  loss_ce: 13.2338 (12.9843)  loss_bbox: 2.1680 (2.1311)  loss_giou: 1.2603 (1.2348)  loss_conf: 0.0772 (0.0825)  loss_ce_unscaled: 2.6468 (2.5969)  class_error_unscaled: 62.0690 (62.9177)  loss_bbox_unscaled: 0.4336 (0.4262)  loss_giou_unscaled: 0.6301 (0.6174)  loss_conf_unscaled: 0.0077 (0.0082)  time: 0.8207  data: 0.1582  max mem: 16066
Test:  [100/213]  eta: 0:01:34  class_error: 67.74  loss: 17.0580 (16.5087)  loss_ce: 13.3143 (13.0465)  loss_bbox: 2.2549 (2.1424)  loss_giou: 1.2604 (1.2372)  loss_conf: 0.0823 (0.0826)  loss_ce_unscaled: 2.6629 (2.6093)  class_error_unscaled: 62.6506 (63.1757)  loss_bbox_unscaled: 0.4510 (0.4285)  loss_giou_unscaled: 0.6302 (0.6186)  loss_conf_unscaled: 0.0082 (0.0083)  time: 0.7595  data: 0.1588  max mem: 16066
Test:  [110/213]  eta: 0:01:25  class_error: 66.30  loss: 16.7194 (16.4949)  loss_ce: 12.9244 (13.0222)  loss_bbox: 2.2302 (2.1496)  loss_giou: 1.2671 (1.2419)  loss_conf: 0.0775 (0.0812)  loss_ce_unscaled: 2.5849 (2.6044)  class_error_unscaled: 64.3564 (63.1398)  loss_bbox_unscaled: 0.4460 (0.4299)  loss_giou_unscaled: 0.6336 (0.6210)  loss_conf_unscaled: 0.0078 (0.0081)  time: 0.7568  data: 0.1570  max mem: 16066
Test:  [120/213]  eta: 0:01:17  class_error: 66.28  loss: 16.7194 (16.5181)  loss_ce: 12.9244 (13.0407)  loss_bbox: 2.1564 (2.1515)  loss_giou: 1.2803 (1.2453)  loss_conf: 0.0672 (0.0806)  loss_ce_unscaled: 2.5849 (2.6081)  class_error_unscaled: 65.4762 (63.3611)  loss_bbox_unscaled: 0.4313 (0.4303)  loss_giou_unscaled: 0.6401 (0.6227)  loss_conf_unscaled: 0.0067 (0.0081)  time: 0.8242  data: 0.1532  max mem: 16066
Test:  [130/213]  eta: 0:01:08  class_error: 65.59  loss: 17.0589 (16.5497)  loss_ce: 13.4128 (13.0599)  loss_bbox: 2.2054 (2.1591)  loss_giou: 1.3112 (1.2510)  loss_conf: 0.0672 (0.0797)  loss_ce_unscaled: 2.6826 (2.6120)  class_error_unscaled: 66.2651 (63.6477)  loss_bbox_unscaled: 0.4411 (0.4318)  loss_giou_unscaled: 0.6556 (0.6255)  loss_conf_unscaled: 0.0067 (0.0080)  time: 0.7800  data: 0.1597  max mem: 16066
Test:  [140/213]  eta: 0:01:00  class_error: 59.79  loss: 16.7531 (16.5330)  loss_ce: 13.1625 (13.0521)  loss_bbox: 2.2054 (2.1535)  loss_giou: 1.2330 (1.2476)  loss_conf: 0.0735 (0.0799)  loss_ce_unscaled: 2.6325 (2.6104)  class_error_unscaled: 64.0449 (63.3239)  loss_bbox_unscaled: 0.4411 (0.4307)  loss_giou_unscaled: 0.6165 (0.6238)  loss_conf_unscaled: 0.0073 (0.0080)  time: 0.7675  data: 0.1585  max mem: 16066
Test:  [150/213]  eta: 0:00:51  class_error: 66.67  loss: 16.3440 (16.5188)  loss_ce: 12.8396 (13.0319)  loss_bbox: 2.1535 (2.1573)  loss_giou: 1.2310 (1.2495)  loss_conf: 0.0751 (0.0800)  loss_ce_unscaled: 2.5679 (2.6064)  class_error_unscaled: 60.2151 (63.3319)  loss_bbox_unscaled: 0.4307 (0.4315)  loss_giou_unscaled: 0.6155 (0.6248)  loss_conf_unscaled: 0.0075 (0.0080)  time: 0.7553  data: 0.1509  max mem: 16066
Test:  [160/213]  eta: 0:00:43  class_error: 55.43  loss: 16.4474 (16.5171)  loss_ce: 13.0076 (13.0345)  loss_bbox: 2.1535 (2.1532)  loss_giou: 1.2379 (1.2492)  loss_conf: 0.0762 (0.0803)  loss_ce_unscaled: 2.6015 (2.6069)  class_error_unscaled: 60.2151 (63.1767)  loss_bbox_unscaled: 0.4307 (0.4306)  loss_giou_unscaled: 0.6190 (0.6246)  loss_conf_unscaled: 0.0076 (0.0080)  time: 0.7557  data: 0.1481  max mem: 16066
Test:  [170/213]  eta: 0:00:34  class_error: 62.07  loss: 16.4474 (16.4897)  loss_ce: 13.0076 (13.0158)  loss_bbox: 2.0865 (2.1476)  loss_giou: 1.2238 (1.2463)  loss_conf: 0.0764 (0.0799)  loss_ce_unscaled: 2.6015 (2.6032)  class_error_unscaled: 60.2151 (63.0247)  loss_bbox_unscaled: 0.4173 (0.4295)  loss_giou_unscaled: 0.6119 (0.6232)  loss_conf_unscaled: 0.0076 (0.0080)  time: 0.7619  data: 0.1488  max mem: 16066
Test:  [180/213]  eta: 0:00:26  class_error: 65.00  loss: 16.4600 (16.5168)  loss_ce: 13.2369 (13.0455)  loss_bbox: 2.0865 (2.1432)  loss_giou: 1.2157 (1.2473)  loss_conf: 0.0764 (0.0809)  loss_ce_unscaled: 2.6474 (2.6091)  class_error_unscaled: 61.7284 (63.1599)  loss_bbox_unscaled: 0.4173 (0.4286)  loss_giou_unscaled: 0.6078 (0.6236)  loss_conf_unscaled: 0.0076 (0.0081)  time: 0.7757  data: 0.1701  max mem: 16066
Test:  [190/213]  eta: 0:00:18  class_error: 69.57  loss: 16.7580 (16.5431)  loss_ce: 13.2369 (13.0694)  loss_bbox: 2.0738 (2.1452)  loss_giou: 1.2428 (1.2476)  loss_conf: 0.0835 (0.0809)  loss_ce_unscaled: 2.6474 (2.6139)  class_error_unscaled: 64.3564 (63.1693)  loss_bbox_unscaled: 0.4148 (0.4290)  loss_giou_unscaled: 0.6214 (0.6238)  loss_conf_unscaled: 0.0083 (0.0081)  time: 0.7724  data: 0.1688  max mem: 16066
Test:  [200/213]  eta: 0:00:10  class_error: 64.84  loss: 16.7853 (16.5265)  loss_ce: 13.2358 (13.0541)  loss_bbox: 2.1574 (2.1443)  loss_giou: 1.2368 (1.2472)  loss_conf: 0.0801 (0.0808)  loss_ce_unscaled: 2.6472 (2.6108)  class_error_unscaled: 64.8352 (63.2299)  loss_bbox_unscaled: 0.4315 (0.4289)  loss_giou_unscaled: 0.6184 (0.6236)  loss_conf_unscaled: 0.0080 (0.0081)  time: 0.7930  data: 0.1549  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 63.64  loss: 16.6894 (16.5488)  loss_ce: 13.0276 (13.0819)  loss_bbox: 2.0330 (2.1398)  loss_giou: 1.2208 (1.2465)  loss_conf: 0.0727 (0.0805)  loss_ce_unscaled: 2.6055 (2.6164)  class_error_unscaled: 64.8352 (63.3031)  loss_bbox_unscaled: 0.4066 (0.4280)  loss_giou_unscaled: 0.6104 (0.6233)  loss_conf_unscaled: 0.0073 (0.0080)  time: 0.8000  data: 0.1577  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 66.67  loss: 16.1758 (16.5454)  loss_ce: 12.7755 (13.0810)  loss_bbox: 2.0099 (2.1376)  loss_giou: 1.2183 (1.2464)  loss_conf: 0.0727 (0.0805)  loss_ce_unscaled: 2.5551 (2.6162)  class_error_unscaled: 64.8352 (63.3650)  loss_bbox_unscaled: 0.4020 (0.4275)  loss_giou_unscaled: 0.6092 (0.6232)  loss_conf_unscaled: 0.0073 (0.0080)  time: 0.7892  data: 0.1585  max mem: 16066
Test: Total time: 0:02:51 (0.8035 s / it)
Averaged stats: class_error: 66.67  loss: 16.1758 (16.5454)  loss_ce: 12.7755 (13.0810)  loss_bbox: 2.0099 (2.1376)  loss_giou: 1.2183 (1.2464)  loss_conf: 0.0727 (0.0805)  loss_ce_unscaled: 2.5551 (2.6162)  class_error_unscaled: 64.8352 (63.3650)  loss_bbox_unscaled: 0.4020 (0.4275)  loss_giou_unscaled: 0.6092 (0.6232)  loss_conf_unscaled: 0.0073 (0.0080)
zero-shot mAP: 12.43
rare mAP: 17.20
nonrare mAP: 24.28
full mAP: 17.67

================== checkpoint0080.pth 结束：2025-09-11 09:50:09.506796 ==================

================== checkpoint0075.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0075.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:20:32  class_error: 57.78  loss: 15.4037 (15.4037)  loss_ce: 11.9332 (11.9332)  loss_bbox: 2.1271 (2.1271)  loss_giou: 1.2708 (1.2708)  loss_conf: 0.0725 (0.0725)  loss_ce_unscaled: 2.3866 (2.3866)  class_error_unscaled: 57.7778 (57.7778)  loss_bbox_unscaled: 0.4254 (0.4254)  loss_giou_unscaled: 0.6354 (0.6354)  loss_conf_unscaled: 0.0073 (0.0073)  time: 5.7861  data: 2.4284  max mem: 16066
Test:  [ 10/213]  eta: 0:04:32  class_error: 53.19  loss: 16.5929 (16.6161)  loss_ce: 13.1569 (13.0443)  loss_bbox: 2.2752 (2.1936)  loss_giou: 1.2907 (1.2959)  loss_conf: 0.0725 (0.0823)  loss_ce_unscaled: 2.6314 (2.6089)  class_error_unscaled: 62.5000 (61.9926)  loss_bbox_unscaled: 0.4550 (0.4387)  loss_giou_unscaled: 0.6454 (0.6480)  loss_conf_unscaled: 0.0073 (0.0082)  time: 1.3436  data: 0.4071  max mem: 16066
Test:  [ 20/213]  eta: 0:03:53  class_error: 76.29  loss: 16.5929 (16.8035)  loss_ce: 13.3029 (13.3040)  loss_bbox: 2.1470 (2.1569)  loss_giou: 1.2701 (1.2580)  loss_conf: 0.0733 (0.0845)  loss_ce_unscaled: 2.6606 (2.6608)  class_error_unscaled: 62.5000 (62.8858)  loss_bbox_unscaled: 0.4294 (0.4314)  loss_giou_unscaled: 0.6351 (0.6290)  loss_conf_unscaled: 0.0073 (0.0084)  time: 0.9827  data: 0.2837  max mem: 16066
Test:  [ 30/213]  eta: 0:03:29  class_error: 57.95  loss: 16.7673 (16.6806)  loss_ce: 13.1813 (13.2417)  loss_bbox: 2.0547 (2.1263)  loss_giou: 1.1977 (1.2318)  loss_conf: 0.0705 (0.0808)  loss_ce_unscaled: 2.6363 (2.6483)  class_error_unscaled: 61.2245 (62.9233)  loss_bbox_unscaled: 0.4109 (0.4253)  loss_giou_unscaled: 0.5989 (0.6159)  loss_conf_unscaled: 0.0070 (0.0081)  time: 1.0348  data: 0.3115  max mem: 16066
Test:  [ 40/213]  eta: 0:03:12  class_error: 59.57  loss: 16.3440 (16.6657)  loss_ce: 13.0914 (13.2443)  loss_bbox: 2.0801 (2.1198)  loss_giou: 1.1913 (1.2230)  loss_conf: 0.0672 (0.0786)  loss_ce_unscaled: 2.6183 (2.6489)  class_error_unscaled: 63.2653 (63.5418)  loss_bbox_unscaled: 0.4160 (0.4240)  loss_giou_unscaled: 0.5956 (0.6115)  loss_conf_unscaled: 0.0067 (0.0079)  time: 1.0143  data: 0.2763  max mem: 16066
Test:  [ 50/213]  eta: 0:02:58  class_error: 59.57  loss: 16.2034 (16.7274)  loss_ce: 13.1448 (13.3010)  loss_bbox: 2.0801 (2.1197)  loss_giou: 1.2165 (1.2269)  loss_conf: 0.0779 (0.0797)  loss_ce_unscaled: 2.6290 (2.6602)  class_error_unscaled: 62.2222 (62.9972)  loss_bbox_unscaled: 0.4160 (0.4239)  loss_giou_unscaled: 0.6083 (0.6135)  loss_conf_unscaled: 0.0078 (0.0080)  time: 1.0237  data: 0.2947  max mem: 16066
Test:  [ 60/213]  eta: 0:02:45  class_error: 63.44  loss: 16.7743 (16.7265)  loss_ce: 13.3913 (13.3135)  loss_bbox: 1.9991 (2.1068)  loss_giou: 1.2392 (1.2257)  loss_conf: 0.0779 (0.0806)  loss_ce_unscaled: 2.6783 (2.6627)  class_error_unscaled: 60.0000 (62.8230)  loss_bbox_unscaled: 0.3998 (0.4214)  loss_giou_unscaled: 0.6196 (0.6129)  loss_conf_unscaled: 0.0078 (0.0081)  time: 1.0118  data: 0.2981  max mem: 16066
Test:  [ 70/213]  eta: 0:02:36  class_error: 64.04  loss: 16.8167 (16.7965)  loss_ce: 13.4240 (13.3893)  loss_bbox: 2.0363 (2.1040)  loss_giou: 1.2287 (1.2226)  loss_conf: 0.0738 (0.0806)  loss_ce_unscaled: 2.6848 (2.6779)  class_error_unscaled: 63.4409 (63.0155)  loss_bbox_unscaled: 0.4073 (0.4208)  loss_giou_unscaled: 0.6144 (0.6113)  loss_conf_unscaled: 0.0074 (0.0081)  time: 1.0833  data: 0.3206  max mem: 16066
Test:  [ 80/213]  eta: 0:02:25  class_error: 56.38  loss: 16.7771 (16.8006)  loss_ce: 13.1445 (13.3902)  loss_bbox: 2.1161 (2.1040)  loss_giou: 1.2039 (1.2253)  loss_conf: 0.0738 (0.0811)  loss_ce_unscaled: 2.6289 (2.6780)  class_error_unscaled: 64.0449 (63.2683)  loss_bbox_unscaled: 0.4232 (0.4208)  loss_giou_unscaled: 0.6020 (0.6127)  loss_conf_unscaled: 0.0074 (0.0081)  time: 1.1175  data: 0.3272  max mem: 16066
Test:  [ 90/213]  eta: 0:02:13  class_error: 54.44  loss: 16.7843 (16.8782)  loss_ce: 13.1445 (13.4390)  loss_bbox: 2.2275 (2.1245)  loss_giou: 1.2845 (1.2323)  loss_conf: 0.0859 (0.0824)  loss_ce_unscaled: 2.6289 (2.6878)  class_error_unscaled: 63.0435 (63.2254)  loss_bbox_unscaled: 0.4455 (0.4249)  loss_giou_unscaled: 0.6422 (0.6161)  loss_conf_unscaled: 0.0086 (0.0082)  time: 1.0395  data: 0.2844  max mem: 16066
Test:  [100/213]  eta: 0:02:02  class_error: 65.59  loss: 17.4813 (16.9431)  loss_ce: 13.8789 (13.4825)  loss_bbox: 2.3539 (2.1408)  loss_giou: 1.2845 (1.2379)  loss_conf: 0.0818 (0.0819)  loss_ce_unscaled: 2.7758 (2.6965)  class_error_unscaled: 64.8936 (63.4541)  loss_bbox_unscaled: 0.4708 (0.4282)  loss_giou_unscaled: 0.6422 (0.6190)  loss_conf_unscaled: 0.0082 (0.0082)  time: 1.0662  data: 0.2677  max mem: 16066
Test:  [110/213]  eta: 0:01:50  class_error: 63.04  loss: 17.0832 (16.8962)  loss_ce: 13.4637 (13.4273)  loss_bbox: 2.2559 (2.1458)  loss_giou: 1.2821 (1.2421)  loss_conf: 0.0746 (0.0809)  loss_ce_unscaled: 2.6927 (2.6855)  class_error_unscaled: 64.7059 (63.2572)  loss_bbox_unscaled: 0.4512 (0.4292)  loss_giou_unscaled: 0.6410 (0.6211)  loss_conf_unscaled: 0.0075 (0.0081)  time: 1.0445  data: 0.2928  max mem: 16066
Test:  [120/213]  eta: 0:01:39  class_error: 67.44  loss: 16.6748 (16.9350)  loss_ce: 13.3343 (13.4543)  loss_bbox: 2.2242 (2.1534)  loss_giou: 1.2885 (1.2475)  loss_conf: 0.0688 (0.0798)  loss_ce_unscaled: 2.6669 (2.6909)  class_error_unscaled: 63.4409 (63.4455)  loss_bbox_unscaled: 0.4448 (0.4307)  loss_giou_unscaled: 0.6443 (0.6238)  loss_conf_unscaled: 0.0069 (0.0080)  time: 0.9828  data: 0.2529  max mem: 16066
Test:  [130/213]  eta: 0:01:28  class_error: 63.44  loss: 17.2805 (16.9421)  loss_ce: 13.3343 (13.4490)  loss_bbox: 2.2387 (2.1610)  loss_giou: 1.3235 (1.2529)  loss_conf: 0.0668 (0.0792)  loss_ce_unscaled: 2.6669 (2.6898)  class_error_unscaled: 66.3265 (63.7382)  loss_bbox_unscaled: 0.4477 (0.4322)  loss_giou_unscaled: 0.6617 (0.6265)  loss_conf_unscaled: 0.0067 (0.0079)  time: 1.0140  data: 0.2018  max mem: 16066
Test:  [140/213]  eta: 0:01:16  class_error: 64.95  loss: 16.8947 (16.9566)  loss_ce: 13.1582 (13.4685)  loss_bbox: 2.2297 (2.1579)  loss_giou: 1.2272 (1.2508)  loss_conf: 0.0743 (0.0794)  loss_ce_unscaled: 2.6316 (2.6937)  class_error_unscaled: 64.9485 (63.5507)  loss_bbox_unscaled: 0.4459 (0.4316)  loss_giou_unscaled: 0.6136 (0.6254)  loss_conf_unscaled: 0.0074 (0.0079)  time: 0.8999  data: 0.1952  max mem: 16066
Test:  [150/213]  eta: 0:01:05  class_error: 65.56  loss: 16.6922 (16.9588)  loss_ce: 13.1761 (13.4679)  loss_bbox: 2.1950 (2.1598)  loss_giou: 1.2175 (1.2517)  loss_conf: 0.0806 (0.0794)  loss_ce_unscaled: 2.6352 (2.6936)  class_error_unscaled: 62.3656 (63.5685)  loss_bbox_unscaled: 0.4390 (0.4320)  loss_giou_unscaled: 0.6087 (0.6259)  loss_conf_unscaled: 0.0081 (0.0079)  time: 0.8952  data: 0.3140  max mem: 16066
Test:  [160/213]  eta: 0:00:54  class_error: 54.35  loss: 16.6922 (16.9270)  loss_ce: 13.2012 (13.4407)  loss_bbox: 2.1842 (2.1552)  loss_giou: 1.2230 (1.2514)  loss_conf: 0.0806 (0.0797)  loss_ce_unscaled: 2.6402 (2.6881)  class_error_unscaled: 64.2857 (63.5260)  loss_bbox_unscaled: 0.4368 (0.4310)  loss_giou_unscaled: 0.6115 (0.6257)  loss_conf_unscaled: 0.0081 (0.0080)  time: 0.8873  data: 0.3122  max mem: 16066
Test:  [170/213]  eta: 0:00:44  class_error: 62.07  loss: 15.9643 (16.8661)  loss_ce: 12.7791 (13.3888)  loss_bbox: 2.1063 (2.1495)  loss_giou: 1.2286 (1.2482)  loss_conf: 0.0774 (0.0795)  loss_ce_unscaled: 2.5558 (2.6778)  class_error_unscaled: 63.2653 (63.4259)  loss_bbox_unscaled: 0.4213 (0.4299)  loss_giou_unscaled: 0.6143 (0.6241)  loss_conf_unscaled: 0.0077 (0.0080)  time: 0.9442  data: 0.2016  max mem: 16066
Test:  [180/213]  eta: 0:00:33  class_error: 68.00  loss: 16.2475 (16.8840)  loss_ce: 12.9094 (13.4060)  loss_bbox: 2.0696 (2.1481)  loss_giou: 1.2286 (1.2500)  loss_conf: 0.0710 (0.0798)  loss_ce_unscaled: 2.5819 (2.6812)  class_error_unscaled: 62.3762 (63.5484)  loss_bbox_unscaled: 0.4139 (0.4296)  loss_giou_unscaled: 0.6143 (0.6250)  loss_conf_unscaled: 0.0071 (0.0080)  time: 1.0126  data: 0.2377  max mem: 16066
Test:  [190/213]  eta: 0:00:23  class_error: 68.48  loss: 16.9070 (16.8958)  loss_ce: 13.6194 (13.4203)  loss_bbox: 2.0860 (2.1457)  loss_giou: 1.2358 (1.2493)  loss_conf: 0.0911 (0.0805)  loss_ce_unscaled: 2.7239 (2.6841)  class_error_unscaled: 64.1304 (63.5377)  loss_bbox_unscaled: 0.4172 (0.4291)  loss_giou_unscaled: 0.6179 (0.6246)  loss_conf_unscaled: 0.0091 (0.0080)  time: 1.0327  data: 0.2518  max mem: 16066
Test:  [200/213]  eta: 0:00:13  class_error: 65.93  loss: 16.6912 (16.8828)  loss_ce: 13.2573 (13.4089)  loss_bbox: 2.0860 (2.1435)  loss_giou: 1.2358 (1.2495)  loss_conf: 0.0797 (0.0809)  loss_ce_unscaled: 2.6515 (2.6818)  class_error_unscaled: 64.5833 (63.5938)  loss_bbox_unscaled: 0.4172 (0.4287)  loss_giou_unscaled: 0.6179 (0.6248)  loss_conf_unscaled: 0.0080 (0.0081)  time: 0.9965  data: 0.2365  max mem: 16066
Test:  [210/213]  eta: 0:00:03  class_error: 62.50  loss: 16.9970 (16.9188)  loss_ce: 13.3691 (13.4518)  loss_bbox: 2.0160 (2.1373)  loss_giou: 1.2168 (1.2488)  loss_conf: 0.0795 (0.0810)  loss_ce_unscaled: 2.6738 (2.6904)  class_error_unscaled: 64.5833 (63.6720)  loss_bbox_unscaled: 0.4032 (0.4275)  loss_giou_unscaled: 0.6084 (0.6244)  loss_conf_unscaled: 0.0079 (0.0081)  time: 0.8323  data: 0.2194  max mem: 16066
Test:  [212/213]  eta: 0:00:01  class_error: 62.96  loss: 16.8280 (16.9065)  loss_ce: 13.3691 (13.4420)  loss_bbox: 1.9773 (2.1349)  loss_giou: 1.2131 (1.2487)  loss_conf: 0.0797 (0.0809)  loss_ce_unscaled: 2.6738 (2.6884)  class_error_unscaled: 64.2857 (63.7030)  loss_bbox_unscaled: 0.3955 (0.4270)  loss_giou_unscaled: 0.6066 (0.6244)  loss_conf_unscaled: 0.0080 (0.0081)  time: 0.9586  data: 0.2217  max mem: 16066
Test: Total time: 0:03:37 (1.0226 s / it)
Averaged stats: class_error: 62.96  loss: 16.8280 (16.9065)  loss_ce: 13.3691 (13.4420)  loss_bbox: 1.9773 (2.1349)  loss_giou: 1.2131 (1.2487)  loss_conf: 0.0797 (0.0809)  loss_ce_unscaled: 2.6738 (2.6884)  class_error_unscaled: 64.2857 (63.7030)  loss_bbox_unscaled: 0.3955 (0.4270)  loss_giou_unscaled: 0.6066 (0.6244)  loss_conf_unscaled: 0.0080 (0.0081)
zero-shot mAP: 12.02
rare mAP: 16.54
nonrare mAP: 23.62
full mAP: 17.08

================== checkpoint0075.pth 结束：2025-09-11 09:54:46.149506 ==================

================== checkpoint0070.pth ==================
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
| distributed init (rank 0): env://
git:
  sha: N/A, status: clean, branch: N/A

Namespace(epoch=0, lr=0.0001, lr_backbone=1e-05, batch_size=64, weight_decay=0.0001, epochs=100, lr_drop=120, clip_max_norm=0.1, clip_model='ViT-B/16', description_file_path='swig-build-tree-embedding.json', embed_dim=512, image_resolution=224, vision_layers=12, vision_width=768, vision_patch_size=16, hoi_token_length=64, clip_preprocess=True, vision_decoder_layers=4, vision_decoder_heads=8, num_tokens=12, prompt_dim=768, total_d_layer=11, out_indices=[5, 6, 7, 8, 9, 10], get_embeddings=True, multi_scale=False, f_idxs=None, reverse_level_id=False, semantic_query=False, semantic_units_file='', context_length=77, vocab_size=49408, transformer_width=512, transformer_heads=8, transformer_layers=12, prefix_length=8, conjun_length=2, use_aux_text=False, auxiliary_prefix_length=0, use_prompt_hint=False, hoi_dropout_weight=0.1, feature_map_dropout_weight=0.1, enable_dec=True, dec_heads=8, dec_layers=4, fusion_mode='fixed2.0', enable_simple_distillation=False, simple_distillation_weight=0.1, enable_teacher_student_distillation=False, teacher_student_distillation_weight=0.1, distillation_temperature=4.0, soft_loss_weight=0.7, hard_loss_weight=0.3, enable_feature_buffer=False, buffer_size=1000, feature_dim=512, aux_loss=True, enable_focal_loss=True, focal_alpha=0.3, focal_gamma=1.0, set_cost_class=5, set_cost_bbox=5, set_cost_giou=2, set_cost_conf=10, hoi_type='center-dis', set_cost_hoi_type=0, consider_all=False, class_loss_coef=5, bbox_loss_coef=5, giou_loss_coef=2, conf_loss_coef=10, eos_coef=0.1, sched='warmupcos', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-07, warmup_epochs=0, decay_rate=0.1, dataset_file='swig', repeat_factor_sampling=False, zero_shot_exp=True, ignore_non_interaction=True, zero_shot_type='rare_first', enable_softmax=True, test_score_thresh=0.0001, eval_size=448, vis_outputs=False, vis_dir='', bbox_lambda=2.0, aux_text_weight=1.0, best_beta=1.0, eval_subset=False, eval=True, seed=22, resume='', pretrained='./checkpoints/swig_hoi/cn_old_clip_alpha_atten/checkpoint0070.pth', start_epoch=0, output_dir='./checkpoints/swig/test/cn_clip_alpha_newprompt', device='cuda', world_size=1, dist_url='env://', local_rank=None, num_workers=2, rank=0, gpu=0, distributed=False, dist_backend='nccl')
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
number of trainable params: 84024085 84.024M
number of total params: 236407066 236.407M
# train: 54601 , # val 13588
Loading vision model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/ViT-B-16.json
Loading text model config from /home/think/Code/LMHOI_ICASSP/LMHOI-V1.0/Chinese-CLIP/cn_clip/clip/model_configs/RoBERTa-wwm-ext-base-chinese.json
Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}
Test:  [  0/213]  eta: 0:17:10  class_error: 62.22  loss: 16.4621 (16.4621)  loss_ce: 12.9382 (12.9382)  loss_bbox: 2.1391 (2.1391)  loss_giou: 1.3003 (1.3003)  loss_conf: 0.0845 (0.0845)  loss_ce_unscaled: 2.5876 (2.5876)  class_error_unscaled: 62.2222 (62.2222)  loss_bbox_unscaled: 0.4278 (0.4278)  loss_giou_unscaled: 0.6501 (0.6501)  loss_conf_unscaled: 0.0085 (0.0085)  time: 4.8381  data: 2.0651  max mem: 16066
Test:  [ 10/213]  eta: 0:04:13  class_error: 53.19  loss: 17.6871 (17.6868)  loss_ce: 14.3252 (14.0709)  loss_bbox: 2.2336 (2.2245)  loss_giou: 1.3003 (1.3066)  loss_conf: 0.0845 (0.0847)  loss_ce_unscaled: 2.8650 (2.8142)  class_error_unscaled: 62.2222 (62.0046)  loss_bbox_unscaled: 0.4467 (0.4449)  loss_giou_unscaled: 0.6501 (0.6533)  loss_conf_unscaled: 0.0085 (0.0085)  time: 1.2494  data: 0.3673  max mem: 16066
Test:  [ 20/213]  eta: 0:03:33  class_error: 72.16  loss: 17.7268 (17.8230)  loss_ce: 14.3252 (14.3119)  loss_bbox: 2.1134 (2.1567)  loss_giou: 1.2719 (1.2662)  loss_conf: 0.0839 (0.0881)  loss_ce_unscaled: 2.8650 (2.8624)  class_error_unscaled: 63.2184 (63.5448)  loss_bbox_unscaled: 0.4227 (0.4313)  loss_giou_unscaled: 0.6360 (0.6331)  loss_conf_unscaled: 0.0084 (0.0088)  time: 0.9170  data: 0.1875  max mem: 16066
Test:  [ 30/213]  eta: 0:03:11  class_error: 60.23  loss: 17.7268 (17.6898)  loss_ce: 14.1517 (14.2185)  loss_bbox: 2.0986 (2.1427)  loss_giou: 1.2035 (1.2442)  loss_conf: 0.0754 (0.0844)  loss_ce_unscaled: 2.8303 (2.8437)  class_error_unscaled: 62.2449 (63.5653)  loss_bbox_unscaled: 0.4197 (0.4285)  loss_giou_unscaled: 0.6017 (0.6221)  loss_conf_unscaled: 0.0075 (0.0084)  time: 0.9308  data: 0.2123  max mem: 16066
Test:  [ 40/213]  eta: 0:03:03  class_error: 59.57  loss: 17.2897 (17.6308)  loss_ce: 14.1734 (14.1832)  loss_bbox: 2.0911 (2.1296)  loss_giou: 1.2082 (1.2356)  loss_conf: 0.0732 (0.0824)  loss_ce_unscaled: 2.8347 (2.8366)  class_error_unscaled: 61.9565 (63.9228)  loss_bbox_unscaled: 0.4182 (0.4259)  loss_giou_unscaled: 0.6041 (0.6178)  loss_conf_unscaled: 0.0073 (0.0082)  time: 1.0204  data: 0.2532  max mem: 16066
Test:  [ 50/213]  eta: 0:02:51  class_error: 60.64  loss: 17.4840 (17.6976)  loss_ce: 14.3324 (14.2513)  loss_bbox: 2.0848 (2.1269)  loss_giou: 1.2194 (1.2377)  loss_conf: 0.0738 (0.0816)  loss_ce_unscaled: 2.8665 (2.8503)  class_error_unscaled: 61.1111 (63.3710)  loss_bbox_unscaled: 0.4170 (0.4254)  loss_giou_unscaled: 0.6097 (0.6189)  loss_conf_unscaled: 0.0074 (0.0082)  time: 1.0669  data: 0.2555  max mem: 16066
Test:  [ 60/213]  eta: 0:02:41  class_error: 60.22  loss: 17.4596 (17.6365)  loss_ce: 14.5291 (14.1961)  loss_bbox: 2.0318 (2.1207)  loss_giou: 1.2747 (1.2381)  loss_conf: 0.0746 (0.0816)  loss_ce_unscaled: 2.9058 (2.8392)  class_error_unscaled: 60.6383 (63.0849)  loss_bbox_unscaled: 0.4064 (0.4241)  loss_giou_unscaled: 0.6374 (0.6190)  loss_conf_unscaled: 0.0075 (0.0082)  time: 1.0436  data: 0.2795  max mem: 16066
Test:  [ 70/213]  eta: 0:02:29  class_error: 61.80  loss: 17.9452 (17.7315)  loss_ce: 14.5291 (14.2938)  loss_bbox: 2.0787 (2.1204)  loss_giou: 1.2293 (1.2356)  loss_conf: 0.0821 (0.0816)  loss_ce_unscaled: 2.9058 (2.8588)  class_error_unscaled: 61.4583 (63.1633)  loss_bbox_unscaled: 0.4157 (0.4241)  loss_giou_unscaled: 0.6146 (0.6178)  loss_conf_unscaled: 0.0082 (0.0082)  time: 1.0254  data: 0.2621  max mem: 16066
Test:  [ 80/213]  eta: 0:02:16  class_error: 55.32  loss: 17.9452 (17.7596)  loss_ce: 14.4157 (14.3228)  loss_bbox: 2.1255 (2.1158)  loss_giou: 1.2613 (1.2392)  loss_conf: 0.0773 (0.0819)  loss_ce_unscaled: 2.8831 (2.8646)  class_error_unscaled: 63.5417 (63.3888)  loss_bbox_unscaled: 0.4251 (0.4232)  loss_giou_unscaled: 0.6306 (0.6196)  loss_conf_unscaled: 0.0077 (0.0082)  time: 0.9330  data: 0.1988  max mem: 16066
Test:  [ 90/213]  eta: 0:02:08  class_error: 58.89  loss: 18.2416 (17.8531)  loss_ce: 14.7291 (14.3916)  loss_bbox: 2.1655 (2.1340)  loss_giou: 1.2824 (1.2450)  loss_conf: 0.0778 (0.0826)  loss_ce_unscaled: 2.9458 (2.8783)  class_error_unscaled: 63.2075 (63.4161)  loss_bbox_unscaled: 0.4331 (0.4268)  loss_giou_unscaled: 0.6412 (0.6225)  loss_conf_unscaled: 0.0078 (0.0083)  time: 1.0573  data: 0.2615  max mem: 16066
Test:  [100/213]  eta: 0:01:59  class_error: 67.74  loss: 18.7231 (17.8958)  loss_ce: 15.0087 (14.4194)  loss_bbox: 2.2456 (2.1442)  loss_giou: 1.2649 (1.2495)  loss_conf: 0.0833 (0.0827)  loss_ce_unscaled: 3.0017 (2.8839)  class_error_unscaled: 62.9213 (63.5404)  loss_bbox_unscaled: 0.4491 (0.4288)  loss_giou_unscaled: 0.6325 (0.6248)  loss_conf_unscaled: 0.0083 (0.0083)  time: 1.1708  data: 0.3123  max mem: 16066
Test:  [110/213]  eta: 0:01:46  class_error: 66.30  loss: 17.5056 (17.8619)  loss_ce: 13.5630 (14.3717)  loss_bbox: 2.2456 (2.1541)  loss_giou: 1.2765 (1.2550)  loss_conf: 0.0682 (0.0812)  loss_ce_unscaled: 2.7126 (2.8743)  class_error_unscaled: 61.6279 (63.2376)  loss_bbox_unscaled: 0.4491 (0.4308)  loss_giou_unscaled: 0.6383 (0.6275)  loss_conf_unscaled: 0.0068 (0.0081)  time: 0.9729  data: 0.2424  max mem: 16066
Test:  [120/213]  eta: 0:01:35  class_error: 66.28  loss: 17.7026 (17.8765)  loss_ce: 13.9761 (14.3688)  loss_bbox: 2.3160 (2.1663)  loss_giou: 1.3202 (1.2615)  loss_conf: 0.0667 (0.0799)  loss_ce_unscaled: 2.7952 (2.8738)  class_error_unscaled: 63.6364 (63.3943)  loss_bbox_unscaled: 0.4632 (0.4333)  loss_giou_unscaled: 0.6601 (0.6308)  loss_conf_unscaled: 0.0067 (0.0080)  time: 0.8896  data: 0.2704  max mem: 16066
Test:  [130/213]  eta: 0:01:24  class_error: 65.59  loss: 17.8480 (17.8757)  loss_ce: 13.9885 (14.3568)  loss_bbox: 2.3184 (2.1731)  loss_giou: 1.3275 (1.2671)  loss_conf: 0.0667 (0.0788)  loss_ce_unscaled: 2.7977 (2.8714)  class_error_unscaled: 67.0455 (63.8601)  loss_bbox_unscaled: 0.4637 (0.4346)  loss_giou_unscaled: 0.6638 (0.6335)  loss_conf_unscaled: 0.0067 (0.0079)  time: 0.9651  data: 0.2553  max mem: 16066
Test:  [140/213]  eta: 0:01:13  class_error: 57.73  loss: 17.6148 (17.8827)  loss_ce: 14.0079 (14.3706)  loss_bbox: 2.1861 (2.1686)  loss_giou: 1.2288 (1.2646)  loss_conf: 0.0781 (0.0789)  loss_ce_unscaled: 2.8016 (2.8741)  class_error_unscaled: 64.5161 (63.5632)  loss_bbox_unscaled: 0.4372 (0.4337)  loss_giou_unscaled: 0.6144 (0.6323)  loss_conf_unscaled: 0.0078 (0.0079)  time: 0.8946  data: 0.1816  max mem: 16066
Test:  [150/213]  eta: 0:01:03  class_error: 71.11  loss: 17.6148 (17.8760)  loss_ce: 14.0079 (14.3562)  loss_bbox: 2.1861 (2.1737)  loss_giou: 1.2673 (1.2674)  loss_conf: 0.0737 (0.0787)  loss_ce_unscaled: 2.8016 (2.8712)  class_error_unscaled: 61.6162 (63.5897)  loss_bbox_unscaled: 0.4372 (0.4347)  loss_giou_unscaled: 0.6336 (0.6337)  loss_conf_unscaled: 0.0074 (0.0079)  time: 0.9214  data: 0.2081  max mem: 16066
Test:  [160/213]  eta: 0:00:52  class_error: 60.87  loss: 17.6910 (17.8521)  loss_ce: 14.3409 (14.3364)  loss_bbox: 2.2610 (2.1704)  loss_giou: 1.2673 (1.2667)  loss_conf: 0.0706 (0.0786)  loss_ce_unscaled: 2.8682 (2.8673)  class_error_unscaled: 65.1685 (63.6020)  loss_bbox_unscaled: 0.4522 (0.4341)  loss_giou_unscaled: 0.6337 (0.6333)  loss_conf_unscaled: 0.0071 (0.0079)  time: 0.9346  data: 0.2264  max mem: 16066
Test:  [170/213]  eta: 0:00:43  class_error: 60.92  loss: 17.7220 (17.8334)  loss_ce: 14.3814 (14.3263)  loss_bbox: 2.0557 (2.1650)  loss_giou: 1.2133 (1.2634)  loss_conf: 0.0730 (0.0787)  loss_ce_unscaled: 2.8763 (2.8653)  class_error_unscaled: 61.9565 (63.5584)  loss_bbox_unscaled: 0.4111 (0.4330)  loss_giou_unscaled: 0.6067 (0.6317)  loss_conf_unscaled: 0.0073 (0.0079)  time: 0.9652  data: 0.2295  max mem: 16066
Test:  [180/213]  eta: 0:00:33  class_error: 62.00  loss: 17.8536 (17.8464)  loss_ce: 14.3853 (14.3375)  loss_bbox: 2.0467 (2.1642)  loss_giou: 1.2324 (1.2658)  loss_conf: 0.0762 (0.0789)  loss_ce_unscaled: 2.8771 (2.8675)  class_error_unscaled: 64.1975 (63.6982)  loss_bbox_unscaled: 0.4093 (0.4328)  loss_giou_unscaled: 0.6162 (0.6329)  loss_conf_unscaled: 0.0076 (0.0079)  time: 1.0142  data: 0.2594  max mem: 16066
Test:  [190/213]  eta: 0:00:23  class_error: 71.74  loss: 18.2455 (17.8540)  loss_ce: 14.6484 (14.3439)  loss_bbox: 2.0375 (2.1653)  loss_giou: 1.2671 (1.2656)  loss_conf: 0.0825 (0.0793)  loss_ce_unscaled: 2.9297 (2.8688)  class_error_unscaled: 64.3564 (63.7335)  loss_bbox_unscaled: 0.4075 (0.4331)  loss_giou_unscaled: 0.6335 (0.6328)  loss_conf_unscaled: 0.0082 (0.0079)  time: 1.0080  data: 0.2435  max mem: 16066
Test:  [200/213]  eta: 0:00:12  class_error: 69.23  loss: 18.1749 (17.8503)  loss_ce: 14.3110 (14.3413)  loss_bbox: 2.1337 (2.1642)  loss_giou: 1.2446 (1.2653)  loss_conf: 0.0825 (0.0795)  loss_ce_unscaled: 2.8622 (2.8683)  class_error_unscaled: 64.2857 (63.8359)  loss_bbox_unscaled: 0.4267 (0.4328)  loss_giou_unscaled: 0.6223 (0.6327)  loss_conf_unscaled: 0.0082 (0.0079)  time: 0.9262  data: 0.1943  max mem: 16066
Test:  [210/213]  eta: 0:00:02  class_error: 63.64  loss: 18.4449 (17.9008)  loss_ce: 14.8600 (14.3970)  loss_bbox: 2.0630 (2.1589)  loss_giou: 1.2592 (1.2654)  loss_conf: 0.0758 (0.0795)  loss_ce_unscaled: 2.9720 (2.8794)  class_error_unscaled: 63.6364 (63.8781)  loss_bbox_unscaled: 0.4126 (0.4318)  loss_giou_unscaled: 0.6296 (0.6327)  loss_conf_unscaled: 0.0076 (0.0079)  time: 0.9361  data: 0.1757  max mem: 16066
Test:  [212/213]  eta: 0:00:00  class_error: 70.37  loss: 18.4449 (17.8899)  loss_ce: 14.8600 (14.3880)  loss_bbox: 2.0210 (2.1569)  loss_giou: 1.2592 (1.2655)  loss_conf: 0.0726 (0.0795)  loss_ce_unscaled: 2.9720 (2.8776)  class_error_unscaled: 64.2857 (63.9469)  loss_bbox_unscaled: 0.4042 (0.4314)  loss_giou_unscaled: 0.6296 (0.6327)  loss_conf_unscaled: 0.0073 (0.0080)  time: 0.9058  data: 0.1658  max mem: 16066
Test: Total time: 0:03:32 (0.9957 s / it)
Averaged stats: class_error: 70.37  loss: 18.4449 (17.8899)  loss_ce: 14.8600 (14.3880)  loss_bbox: 2.0210 (2.1569)  loss_giou: 1.2592 (1.2655)  loss_conf: 0.0726 (0.0795)  loss_ce_unscaled: 2.9720 (2.8776)  class_error_unscaled: 64.2857 (63.9469)  loss_bbox_unscaled: 0.4042 (0.4314)  loss_giou_unscaled: 0.6296 (0.6327)  loss_conf_unscaled: 0.0073 (0.0080)
zero-shot mAP: 11.19
rare mAP: 16.56
nonrare mAP: 23.76
full mAP: 16.91

================== checkpoint0070.pth 结束：2025-09-11 09:59:17.754890 ==================
